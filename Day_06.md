## ğŸ“¦ Poisson Distribution ğŸ“Š

### ğŸ§  What is it?

The **Poisson Distribution** helps us find the **probability of a number of events happening** in a **fixed time or space**, based on an **average rate of occurrence**.

ğŸ‘‰ It is **not** about the number of trials like in Binomial Distribution.
Instead, it deals with **how often something happens** in a continuous frame â€” like **per hour, per km, per area**, etc.

---

### ğŸ” Poisson vs Binomial â€“ Key Difference

| Feature         | Binomial Distribution ğŸ¯         | Poisson Distribution â±ï¸                    |
| --------------- | -------------------------------- | ------------------------------------------ |
| Trials          | Fixed number of trials (n)       | No fixed trials, but time/space interval   |
| Success Count   | Out of total trials (n)          | Number of events in time/distance          |
| Avg Rate Symbol | Probability of success (p)       | Average events per interval (Î» - Lambda)   |
| Success Symbol  | x or k                           | x                                          |
| Example         | 10 coin tosses â†’ how many heads? | Avg 10 pages/hour â†’ how many in next hour? |

---

### ğŸ“˜ Real-World Example

> Suppose you **usually read 10 pages per hour**.
> Whatâ€™s the chance youâ€™ll read **exactly 8 pages** in the next hour?

Here,

* Î» (lambda) = 10 (average pages per hour)
* x = 8 (desired outcome: 8 pages in an hour)

### ğŸ§® Formula for Poisson Probability

$$
P(x) = \frac{{Î»^x \cdot e^{-Î»}}}{{x!}}
$$

Where:

* **P(x)** = Probability of x events occurring
* **Î»** = Average number of events in the interval
* **x** = Actual number of events
* **e** = Eulerâ€™s constant â‰ˆ 2.718
* **x!** = x factorial (e.g., 4! = 4Ã—3Ã—2Ã—1)

---

### ğŸ”¢ Plug-in Example:

$$
P(8) = \frac{{10^8 \cdot e^{-10}}}{{8!}} = \frac{{100000000 \cdot 2.718^{-10}}}{{40320}} \approx 0.113
$$

âœ… **Answer**: Thereâ€™s about an **11% chance** that youâ€™ll read exactly **8 pages** in the next hour.

ğŸ§  Tip: You can use calculators or tools like Excel or Python for big factorials and exponentials.

---

### ğŸ“ Summary Table

| Term           | Meaning                               |
| -------------- | ------------------------------------- |
| **x**          | Number of successes/events you want   |
| **Î» (Lambda)** | Average number of events per interval |
| **e**          | Eulerâ€™s constant â‰ˆ 2.718              |
| **x!**         | Factorial of x (multiply from x to 1) |

---

ğŸ” You can think of **Poisson** as a model for:

* ğŸ“ Number of phone calls at a call center per hour
* ğŸŒ§ï¸ Number of rainy days in a month
* ğŸ“¦ Packages arriving at a warehouse per day

---

## ğŸš— Poisson Example: Toll Plaza Scenario ğŸ›£ï¸

### ğŸ¯ Problem:

> On average, **15 cars** pass through a toll plaza **every hour**.
> What is the **probability that exactly 13 cars** will pass through in the **next hour**?

---

### ğŸ“Š Given:

* Î» (average rate) = 15 cars/hour
* x (desired outcome) = 13 cars
* e = 2.718 (Eulerâ€™s constant)

---

### ğŸ§® Use the Poisson Formula:

$$
P(x) = \frac{{Î»^x \cdot e^{-Î»}}}{{x!}}
$$

Substituting the values:

$$
P(13) = \frac{{15^{13} \cdot e^{-15}}}{{13!}}
$$

Breakdown:

* $15^{13}$ = 15407021574586368
* $13!$ = 6227020800
* $e^{-15} \approx 3.059 \times 10^{-7}$

So,

$$
P(13) \approx \frac{{15407021574586368 \times 3.059 \times 10^{-7}}}{{6227020800}} \approx 0.0956
$$

---

### âœ… Final Answer:

ğŸ‘‰ There is a **9.56%** chance that **exactly 13 cars** will pass through the toll plaza in the next hour.

---

### ğŸ’¡ Real-World Takeaway:

Poisson distribution helps when:

* The event is **random**
* You know the **average rate (Î»)**
* You're looking for **the probability of a specific count (x)** in a **time/distance/space frame**

---

Perfect! Here's your refined and easy-to-understand comparison of **Binomial vs Poisson Distribution** ğŸ¤âš–ï¸ â€” rewritten for clarity, with real-life context, emojis, and everything laid out for quick revision and deeper understanding.

---

## âš–ï¸ Binomial vs Poisson Distribution ğŸ¯ğŸ”

Both **Binomial** and **Poisson** distributions help us calculate **the probability of certain events happening**, but they are used in **different scenarios**.

Letâ€™s compare them with the example of **students attending lectures** ğŸ‘©â€ğŸ«ğŸ‘¨â€ğŸ«

---

### ğŸ² **Binomial Distribution** â€“ "Individual Chances"

#### ğŸ§© Problem:

> Out of 60 students, **each student has an 80% chance** of attending a lecture.
> Whatâ€™s the probability that **exactly 50 students** will attend?

#### âœ… Given:

* **n** (number of trials/students) = 60
* **p** (probability of success i.e., attending) = 48/60 = 0.80
* **x or k** (number of successes) = 50

#### ğŸ§® Use:

```python
binom.pmf(k=50, n=60, p=0.80)
```

#### ğŸ¯ Why use Binomial?

âœ… Fixed number of students (**n = 60**)
âœ… Each has a success/failure outcome (attend or not)
âœ… Constant probability (**p = 0.8**)
âœ… Weâ€™re asked for **exactly 50** students

ğŸ’¬ Think of it like **flipping a coin** 60 times, and asking how likely it is to get heads (success) 50 times if the coin is biased (p â‰  0.5).

---

### ğŸ§® **Poisson Distribution** â€“ "Overall Rate"

#### ğŸ§© Problem:

> On average, **48 students** attend lectures.
> What is the probability that **exactly 50** students attend a particular lecture?

#### âœ… Given:

* **Î» (lambda)** = 48 (average attendees per lecture)
* **x or k** = 50

#### ğŸ§® Use:

```python
poisson.pmf(k=50, mu=48)
```

#### ğŸ¯ Why use Poisson?

âœ… We're given an **overall average (Î» = 48)**
âœ… We're not tracking **individual students** or **probabilities**
âœ… No need to know about each student's behavior
âœ… Just want the probability of **50 students** showing up based on the average

ğŸ’¬ Think of it like **counting cars** passing a toll gate: you donâ€™t know individual driversâ€™ chances â€” you just know **about 48 cars come each hour** on average.

---

### ğŸ†š Summary Table: Binomial vs Poisson

| Feature           | Binomial ğŸ¯                              | Poisson â±ï¸                         |
| ----------------- | ---------------------------------------- | ---------------------------------- |
| Type of event     | Fixed number of trials                   | Number of events per interval      |
| Based on...       | Individual success probability (p)       | Average rate of occurrence (Î»)     |
| Need to know\...  | n (trials), p (success rate)             | Î» (average events/time)            |
| Example           | 60 students with 80% chance of attending | Avg 48 students per lecture        |
| Output            | Probability of x successes out of n      | Probability of x events in a frame |
| Best used when... | You track each event                     | You just know the average rate     |

---

ğŸ” **Rule of Thumb**:

> If **n is large** and **p is small**, Binomial can even be **approximated by Poisson**!

---

## ğŸ“Œ Poisson Distribution â€“ Key Characteristics ğŸ§®ğŸšŒ

### ğŸ¯ **Goal**:

To **predict the probability** of a certain number of **events occurring in a fixed interval of time, space, or area**, **based on a known average rate**.

---

### ğŸ”‘ Key Characteristics of Poisson Distribution:

| Characteristic                | Explanation                                                               |
| ----------------------------- | ------------------------------------------------------------------------- |
| â±ï¸ **Fixed Interval**         | We count events in a set time or space (e.g., per hour, per km, per day). |
| ğŸ“‰ **Known Average Rate (Î»)** | The average number of events (Î» or lambda) is known beforehand.           |
| ğŸ”— **Independence**           | The events happen **independently** â€” one event doesn't affect another.   |
| ğŸŒŸ **Rare Events**            | Often used for events that donâ€™t happen too frequently (i.e., small Î»).   |
| âœ… **Whole Numbers Only**      | Counts events like 0, 1, 2... (canâ€™t have 2.5 buses or calls)             |

---

### ğŸšŒ Real-Life Example:

> ğŸš On average, **3 buses arrive** at a bus stop **every hour**.
> What is the probability that **exactly 5 buses** will arrive in the next hour?

---

### ğŸ§® Use the Poisson Formula:

$$
P(x) = \frac{{Î»^x \cdot e^{-Î»}}}{{x!}}
$$

Where:

* **Î» (lambda)** = 3 (average buses/hour)
* **x** = 5 (number of events we want)
* **e** = 2.718 (Euler's number)
* **x!** = Factorial of x (5! = 5Ã—4Ã—3Ã—2Ã—1)

So,

$$
P(5) = \frac{{3^5 \cdot e^{-3}}}{{5!}} = \frac{{243 \cdot 0.0498}}{{120}} \approx 0.1008
$$

---

### âœ… Final Answer:

There is roughly a **10.08% chance** that exactly **5 buses** will arrive this hour ğŸšâ±ï¸

---

### ğŸ’¡ Other Real-World Applications:

* ğŸ“ Number of calls to a call center per minute
* ğŸŒ‹ Number of volcanic eruptions per year in a region
* ğŸ’Œ Emails received per hour
* ğŸ§¬ DNA mutations in a fixed length of genome

---

## ğŸš— Poisson Example â€“ Cars at Toll Plaza ğŸ›£ï¸

### ğŸ§© Problem Statement:

> On average, **15 cars** pass through a toll plaza **every hour**.
> What is the probability that **exactly 13 cars** will pass through in the next hour?

---

### ğŸ“Š Given:

* **Î» (lambda)** = 15 (average number of cars/hour)
* **x** = 13 (we want the probability of exactly 13 cars)
* **e** = 2.718 (Euler's constant)
* **x!** = 13! = 6227020800

---

### ğŸ§® Step-by-Step Using the Poisson Formula:

$$
P(x) = \frac{Î»^x \cdot e^{-Î»}}{x!}
$$

$$
P(13) = \frac{15^{13} \cdot e^{-15}}{13!}
$$

Breaking it down:

* $15^{13} = 15407021574586368$
* $e^{-15} \approx 3.059 \times 10^{-7}$
* $13! = 6227020800$

Now:

$$
P(13) = \frac{15407021574586368 \cdot 3.059 \times 10^{-7}}{6227020800} \approx 0.0956
$$

---

### âœ… Final Answer:

ğŸ”¢ There is a **9.56%** chance that **exactly 13 cars** will pass through the toll plaza in the next hour.

---

### ğŸ§  Why Poisson?

* We know the **average number** of events (cars) per hour
* The events are **independent**
* We are not dealing with individual probabilities (like in Binomial)
* It's a **count over time**, not a fixed number of trials

---

## ğŸ“Š Normal Distribution in Real Life ğŸŒ

### ğŸ“Œ What is Normal Distribution?

Also known as **Gaussian Distribution**, the **Normal Distribution** is one of the most important and commonly seen distributions in statistics.

It looks like a **bell curve** ğŸ”” â€” symmetric, with most values clustering around the **mean**.

---

### ğŸŒŸ Real-Life Examples That Follow Normal Distribution:

| Scenario                 | Description                                                          |
| ------------------------ | -------------------------------------------------------------------- |
| ğŸ“ **Heights of people** | Most people are of average height, fewer are very tall or very short |
| âš–ï¸ **Weights of people** | Majority lie near the average, with few at extremes                  |
| â¤ï¸ **Blood pressure**    | Most people have normal blood pressure, extreme cases are rare       |
| ğŸ“ **Test scores**       | In many exams, most students score around the average                |

![Normal Distribution Curve](https://github.com/user-attachments/assets/3b0e8688-973a-4fb8-b427-493d3a6f8163)

---

### ğŸ§  Why is Normal Distribution So Important?

* **Natural**: Many biological and physical measurements follow this distribution
* **Predictable**: Properties like the **68-95-99.7 Rule** help in predictions
* **Foundation**: Itâ€™s the basis for many statistical methods (like confidence intervals, hypothesis testing, z-scores)

---

### ğŸ“‰ What *Doesnâ€™t* Follow Normal Distribution?

Some **non-natural** or **unevenly spread** phenomena **donâ€™t** follow the normal curve:

| Scenario                | Reason                                                                                       |
| ----------------------- | -------------------------------------------------------------------------------------------- |
| ğŸ’¸ **Income of people** | Most income distributions are **right-skewed** â€” very few people earn much more than average |
| ğŸ§¾ **Company profits**  | Some companies earn exceptionally high profits, making the distribution skewed               |

---

### ğŸ” Summary:

| Feature          | Normal Distribution ğŸ“ˆ                |
| ---------------- | ------------------------------------- |
| Shape            | Symmetric bell curve ğŸ””               |
| Center           | Mean = Median = Mode                  |
| Spread           | Defined by **Standard Deviation (Ïƒ)** |
| Area under curve | 100% (total probability)              |
| Key use          | Modeling **natural**, continuous data |

---

## ğŸ“ˆ Normal Distribution â€“ Key Features ğŸ§¬

A **Normal Distribution** is not just a fancy curve â€” it's a powerful tool to understand how data behaves in real life! Here's what makes it special ğŸ‘‡

---

### ğŸ§­ 1. **Symmetry Around the Mean**

* The curve is **perfectly symmetric** ğŸª
* Left and right sides are mirror images
* Most values are **centered around the mean** (average)

---

### ğŸ“ 2. **Mean = Median = Mode**

* The **center of the curve** is where:

  * ğŸ“Š **Mean** (average),
  * ğŸ“ˆ **Median** (middle value),
  * ğŸ“Œ **Mode** (most frequent value) â€” all are **equal**!

> ğŸ¯ This happens *only* in a perfectly normal distribution!

---

### ğŸ”” 3. **Bell-Shaped Curve**

* The curve **rises** smoothly, **peaks at the center**, and then **tapers off**
* Extreme values (very high/low) are rare
* Common values (around the mean) are frequent

![Normal Curve](https://github.com/user-attachments/assets/eb79273e-f407-4249-8705-e942246f8f6e)

---

### ğŸ“ 4. **Empirical Rule (68-95-99.7 Rule)**

This rule tells us **how data is spread** across the curve using **Standard Deviation (Ïƒ)**:

| Range        | Meaning                                      |
| ------------ | -------------------------------------------- |
| ğŸ“Š **68%**   | of values lie within **Â±1 SD** from the mean |
| ğŸ“Š **95%**   | within **Â±2 SD** from the mean               |
| ğŸ“Š **99.7%** | within **Â±3 SD** from the mean               |

> ğŸ’¡ This helps in understanding **how unusual** a value is!

---

### ğŸ§® 5. **Z-Score (Standard Score)**

> **Z-score =** How far is a value from the mean in terms of standard deviations?

**Formula**:

$$
Z = \frac{X - \mu}{\sigma}
$$

Where:

* **X** = data point
* **Î¼ (mu)** = mean
* **Ïƒ (sigma)** = standard deviation

---

### ğŸ” Why Z-Score is Useful?

* Helps **standardize** different data sets
* Tells us whether a score is **above or below average**, and **by how much**
* Useful in comparing scores across different units (e.g., test marks, heights)

---

### ğŸ¯ Real-Life Example:

> If a student scores **85** on a test with a **mean of 70** and **SD of 10**:

$$
Z = \frac{85 - 70}{10} = 1.5
$$

ğŸ“Œ This means the student scored **1.5 standard deviations above the mean** â€” above average!

---

### âœ… Recap Table:

| Feature         | Normal Distribution ğŸ“Š               |
| --------------- | ------------------------------------ |
| Shape           | Bell Curve ğŸ””                        |
| Center          | Mean = Median = Mode                 |
| Spread          | Controlled by Standard Deviation (Ïƒ) |
| 68-95-99.7 Rule | Describes spread of data             |
| Z-Score         | Measures relative position           |

---

## ğŸ§® Z-Score Interpretation ğŸ“Š

### ğŸ“Œ What is a Z-Score?

A **Z-score** (also called **standard score**) tells you **how far** a data point is from the **mean**, measured in **standard deviations (Ïƒ)**.

$$
Z = \frac{X - \mu}{\sigma}
$$

Where:

* **X** = your data value
* **Î¼ (mu)** = mean
* **Ïƒ (sigma)** = standard deviation

---

### ğŸ“Š What Does a Z-Score Tell You?

| Z-Score   | Interpretation      | Meaning                      |
| --------- | ------------------- | ---------------------------- |
| ğŸŸ¢ **0**  | Exactly at the mean | The value is average         |
| ğŸ”µ **+1** | 1 SD above the mean | Slightly higher than average |
| ğŸ”µ **+2** | 2 SDs above         | Much higher than average     |
| ğŸ”µ **+3** | 3 SDs above         | Extremely high (rare)        |
| ğŸ”´ **-1** | 1 SD below the mean | Slightly lower than average  |
| ğŸ”´ **-2** | 2 SDs below         | Much lower                   |
| ğŸ”´ **-3** | 3 SDs below         | Extremely low (rare)         |

> ğŸ“Œ **The higher the Z-score**, the **further** from the mean â€” and the **more unusual** the value.

---

### ğŸ§  Example:

> A student scores **85** on a test where the **mean is 70** and **standard deviation is 10**:

$$
Z = \frac{85 - 70}{10} = 1.5
$$

ğŸ” **Interpretation**:
This student is **1.5 standard deviations above the mean** â€” better than most of the class!

---

### ğŸ“ˆ Visual Aid:

![Z-Score Interpretation](https://github.com/user-attachments/assets/e633de4b-406d-4949-b6aa-8498ed86aa5c)

---

### ğŸš¨ Tip:

* **Z-scores between -2 and +2** are considered **normal**
* **Beyond Â±2** is often seen as **unusual or significant** in analysis

---

## ğŸ“Š Normal Distribution â€“ Example with Baby's Weight ğŸ¼

### ğŸ¯ Scenario:

A baby is just born in the US weighing **2.91 kg** (or **2910 grams**).
The doctor says the baby's weight is **below average**. The mother is now **worried**.

ğŸ‘‰ The question is:
**Is this weight unusually low? Or still within the normal range?**

---

### ğŸ“Œ Given:

* **Mean (Î¼)** = 3480 grams
* **Standard Deviation (Ïƒ)** = 462 grams
* **Baby's weight (X)** = 2910 grams

---

### ğŸ§® Step 1: Apply the Z-Score Formula

$$
Z = \frac{X - \mu}{\sigma}
$$

$$
Z = \frac{2910 - 3480}{462} = \frac{-570}{462} â‰ˆ -1.23
$$

---

### ğŸ“‰ Step 2: Interpretation

âœ… **Z = -1.23**
This means the babyâ€™s weight is **1.23 standard deviations *below* the mean**.

| Z-Score Range                | Interpretation        |
| ---------------------------- | --------------------- |
| Between -2 and +2            | **Normal range** âœ”ï¸   |
| Less than -2 or more than +2 | **Unusual or rare** â— |

ğŸ§  **So, -1.23 is NOT unusual** â€” it's still within the normal variation range.
ğŸ‰ The mother **need not worry**, as this weight is low but still **statistically normal**.

---

### ğŸ’¡ Real-World Insight:

Z-scores help **put data in context**:

* Without it, â€œbelow averageâ€ sounds scary âŒ
* With it, we see it's still normal âœ…

---

## ğŸ“ Using Z-Score to Find Percentile ğŸ¯

Letâ€™s use the **Z-score** to find out **what percentage of babies weigh less** than this one ğŸ‘¶

---

### ğŸ§® Given:

* Z-score = **-1.23**
* We're using a **Z-table** to find the corresponding percentile

ğŸŒ Z-table link: [Z-Table](https://www.z-table.com/)

---

### ğŸ” Step-by-Step:

1. **Go to the Z-table**
   [Click here to open it](https://www.z-table.com/)

2. **Find the row** for **-1.2**

3. **Move across the columns** to the one labeled **0.03**

   (Because Z = **-1.23**, which is -1.2 + 0.03)

4. **Value found = 0.1093** or **10.93%**

---

### ğŸ“Š Interpretation:

âœ… **Approximately 11%** of babies have a **lower weight** than this baby
ğŸ“ This baby is in the **11th percentile**

---

### ğŸ’¡ What does that mean?

* This baby **weighs more than 11%** of newborns
* But still within the **normal range** (not in the extreme low end)

> âš ï¸ Percentile â‰  Grade!
> Being in the 11th percentile here means the baby is lighter than most â€” but **not dangerously underweight** unless medically confirmed.

---

### ğŸ¯ Recap Table:

| Z-Score | Percentile | Meaning                              |
| ------- | ---------- | ------------------------------------ |
| -1.23   | \~11%      | Baby is lighter than 89% of newborns |
| 0       | 50%        | Average (mean)                       |
| +1.0    | \~84%      | Heavier than 84% of newborns         |

---

## ğŸ§  Z-Score Calculations ğŸ§¾

### ğŸ¯ What is a Z-Score?

A **Z-score**, also called the **standard score**, tells us **how far a value is from the average (mean)** in terms of **standard deviations**.

> ğŸ” **In simple words:**
> It shows **how unusual or typical** a data point is in a distribution.

---

### ğŸ§® Z-Score Formula:

$$
Z = \frac{x - \mu}{\sigma}
$$

Where:

* **x** = the data value
* **Î¼ (mu)** = population mean
* **Ïƒ (sigma)** = population standard deviation

---

### ğŸ“Š Interpretation of Z-Score:

| Z-Score        | Meaning                          |
| -------------- | -------------------------------- |
| **0**          | Exactly at the average           |
| **+1**         | 1 SD above the average           |
| **-1**         | 1 SD below the average           |
| **+2**         | 2 SDs above â€“ significantly high |
| **-2**         | 2 SDs below â€“ significantly low  |
| **Â±3 or more** | Rare/extreme values (outliers)   |

---

### ğŸ” Example:

> A test has a mean score of **70** and standard deviation of **10**.
> A student scored **85**.

$$
Z = \frac{85 - 70}{10} = 1.5
$$

âœ… This means the student scored **1.5 standard deviations above average** â€” a very good score!

---

### ğŸ“ˆ Visual Aids:

![image](https://github.com/user-attachments/assets/7c263f9a-af45-4fd7-8754-761e93bc8310)
![image](https://github.com/user-attachments/assets/2900bd23-9b18-4a1a-878b-6b7f3d9e4c07)

---

### ğŸ’¡ Why Z-Scores Matter?

* Helps **compare** different datasets (e.g., marks from different subjects)
* Useful in **standardized testing**, **quality control**, and **outlier detection**
* Helps in finding **percentiles** using Z-tables
* Foundation for **normal distribution analysis** and **hypothesis testing**

---

## ğŸ Z-Score Problem: Runnerâ€™s Performance ğŸ§®

A runner participated in **two races**:

* A **200m** sprint
* A **500m** mid-distance run

We want to know: **In which race did she perform better *relative* to others?**

---

### ğŸ—‚ï¸ Given Data:

| Race | Average Time (Î¼) | Std. Deviation (Ïƒ) | Runnerâ€™s Time (X) |
| ---- | ---------------- | ------------------ | ----------------- |
| 200m | 31 sec           | 1.5 sec            | 28 sec            |
| 500m | 125 sec          | 8.2 sec            | 132 sec           |

---

### ğŸ§® Step 1: Calculate Z-scores

#### ğŸƒâ€â™€ï¸ 200m Race:

$$
Z = \frac{X - \mu}{\sigma} = \frac{28 - 31}{1.5} = \frac{-3}{1.5} = -2.0
$$

âœ… This means: She was **2 standard deviations faster** than the average runner.

---

#### ğŸƒâ€â™€ï¸ 500m Race:

$$
Z = \frac{132 - 125}{8.2} = \frac{7}{8.2} â‰ˆ +0.854
$$

âŒ This means: She was **slower than average**, by about **0.85 standard deviations**.

---

### ğŸ§  Final Interpretation:

| Race | Z-Score | Meaning                    |
| ---- | ------- | -------------------------- |
| 200m | -2.0    | Much better than average âœ… |
| 500m | +0.85   | Slower than average âŒ      |

ğŸš€ **Conclusion:**
She performed **better in the 200m race**, as her Z-score is **lower (more negative)** â€” meaning she was **significantly faster** than the average!

---

### ğŸ’¡ Tip:

> Z-scores allow you to compare performances **across different units, scales, or categories** â€” even if the original numbers arenâ€™t directly comparable.

---

## ğŸ† Top Z-Scores in Cricket Across Eras ğŸ“ˆ

### ğŸ¯ Why Use Z-Scores in Cricket?

Cricket has evolved dramatically â€” different formats, rule changes, fitness levels, and match conditions. So, instead of just looking at **raw stats** (like batting average or runs), **Z-scores** help us compare how exceptional a player was **relative to their peers** in their **own era**.

> ğŸ” **Z-score** = How far a playerâ€™s performance is from the average player of their time.

---

### ğŸ§® Z-Score Formula (Recap):

$$
Z = \frac{X - \mu}{\sigma}
$$

Where:

* **X** = Playerâ€™s stat (e.g., batting average)
* **Î¼** = Average stat of all players in that era
* **Ïƒ** = Standard deviation in that era

---

### ğŸ Example: Batting Average Across Eras

Letâ€™s say we want to compare top batters based on their **Z-scores** in Test cricket.

| Player               | Era       | Avg (X) | Era Avg (Î¼) | Std Dev (Ïƒ) | Z-Score  | Notes                                          |
| -------------------- | --------- | ------- | ----------- | ----------- | -------- | ---------------------------------------------- |
| **Don Bradman**      | 1930s-40s | 99.94   | 35.0        | 10.0        | **+6.5** | All-time highest Z-score in cricket history ğŸ”¥ |
| **Steve Smith**      | 2010s     | 60.0    | 32.5        | 12.0        | +2.29    | Exceptional in modern era                      |
| **Virat Kohli**      | 2010s     | 49.3    | 32.5        | 12.0        | +1.4     | Very good but not extreme                      |
| **Kumar Sangakkara** | 2000s     | 57.4    | 35.0        | 11.0        | +2.04    | Consistent performer                           |

---

### ğŸ¯ Key Takeaways:

* **Don Bradman's Z-score of +6.5** is **off the charts**, meaning he was **6.5 standard deviations better than the average player** of his time â€” an unheard-of level of dominance in any sport! ğŸ¤¯
* Other modern greats like **Steve Smith** and **Sangakkara** also show excellent Z-scores, but none come close to Bradman.
* **Z-score normalizes performance**, removing inflation or deflation from different eras.

---

### âš½ Real-Life Analogy:

Imagine comparing a school topper in 1980 vs. one in 2020. The total marks may have changed, but **Z-score tells us how much better** they were **than their classmates** â€” regardless of the time period.

---

## ğŸ“Š Visualizing Z-Scores ğŸ¯

Z-scores help us understand **how far a value lies from the average**, but the **meaning of a "good" or "bad" Z-score depends on the situation**.

---

### ğŸƒâ€â™€ï¸ Scenario 1: **Lower Z-score is Better**

ğŸ‘‰ *Running race times, delivery time, waiting time, etc.*

In these cases, **lower values mean better performance**, so a **more negative Z-score is better** because it means **faster or shorter time**.

#### ğŸ§® Example:

> A runner completes a race in **28 seconds**, while the average is **31 seconds**, with a standard deviation of **1.5**.
>
> $$
> Z = \frac{28 - 31}{1.5} = -2.0
> $$
>
> âœ… A Z-score of **-2.0** means the runner is **much faster than average**.

---

### ğŸ“š Scenario 2: **Higher Z-score is Better**

ğŸ‘‰ *Marks in exams, sales numbers, profits, etc.*

In these cases, **higher values are good**, so a **higher (positive) Z-score is better** because it means you're **above average**.

#### ğŸ§® Example:

> A student scores **85 marks**, average is **70**, standard deviation is **10**.
>
> $$
> Z = \frac{85 - 70}{10} = +1.5
> $$
>
> âœ… A Z-score of **+1.5** means the student performed **significantly better than peers**.

---

### ğŸ–¼ï¸ Visual Aid:

![image](https://github.com/user-attachments/assets/effd7c84-0a59-4fd8-813a-b8a009facbde)

This chart shows how Z-scores relate to performance. Depending on the scenario, your goal might be to move **left (lower Z)** or **right (higher Z)** on the scale.

---

### ğŸ’¡ Summary Table:

| Context            | Preferred Z-Score | Why?                             |
| ------------------ | ----------------- | -------------------------------- |
| **Race time**      | Lower (âˆ’)         | Faster completion is better â±ï¸   |
| **Marks in exam**  | Higher (+)        | Higher marks are better ğŸ“š       |
| **Delivery time**  | Lower (âˆ’)         | Quick delivery is better ğŸ“¦      |
| **Revenue/profit** | Higher (+)        | More revenue/profit is better ğŸ’° |

---

## ğŸ“ˆ Normal Distribution & Probability

### ğŸ§  What is Standard Normal Distribution?

Itâ€™s just a **normal distribution** that has been **standardized**, meaning:

* **Mean (Î¼) = 0**
* **Standard Deviation (Ïƒ) = 1**

We use it to **simplify probability calculations** by using **Z-scores**.

---

### ğŸ“ Key Properties of Normal Distribution:

* âœ… **Symmetrical** bell-shaped curve
* âœ… **Mean = Median = Mode**
* âœ… **Total area under the curve = 1** (or 100%)

---

### ğŸ“Š Why is Area Important?

The **area under the curve** represents **probability**.
ğŸ‘‰ For example:

* Area to the **left of Z = 1** = probability of getting a value **less than 1 SD above the mean**
* Area between **Z = -1 and Z = 1** = **68%** of the data (Empirical Rule)

---

### ğŸ”„ Z-Score to Probability Mapping

By converting raw scores to **Z-scores**, we can:

* Compare data across different distributions
* Use **Z-tables** or tools to find **probabilities**

ğŸ“Œ Example:
If **Z = 0**, you're **exactly at the mean**.
If **Z = +1**, youâ€™re **one standard deviation above the mean**.

---

### ğŸ–¼ï¸ Visual Aid:

![image](https://github.com/user-attachments/assets/5b0ccf67-cc1b-415f-932a-5259a5379c0c)

This image shows how Z-scores map to the **area under the curve**, which is basically the **chance/probability** that a value falls in that region.

---

### ğŸ” Summary:

| Concept                      | Value/Meaning                      |
| ---------------------------- | ---------------------------------- |
| Standard Normal Distribution | Î¼ = 0, Ïƒ = 1                       |
| Area under the curve         | Total = 1 (100%)                   |
| Z-score                      | Distance from the mean in SD units |
| Probability from Z           | Area under curve up to that Z      |

---

## ğŸ” Is Our Data **Normally Distributed**? ğŸ¤”

Before using techniques like **Z-scores** or assuming data follows a **bell curve**, we need to check if our data actually **fits a normal distribution**.

### âœ… Why This Matters:

Many statistical methods (e.g., Z-test, t-test) **assume normal distribution** of data. So verifying this is important!

---

### ğŸ§ª 1. **Shapiro-Wilk Test** (For â‰¤ 5000 rows)

* Itâ€™s a statistical test to check **normality** of a dataset.
* **p-value > 0.05** â†’ Data is **likely normal**
* **p-value â‰¤ 0.05** â†’ Data is **not normally distributed**

ğŸ“Œ Example:
If `p = 0.07` â†’ Looks good! âœ…
If `p = 0.01` â†’ Not normal! âŒ

ğŸ“ Use this when your dataset has **up to 5000 rows**.

---

### ğŸ§ª 2. **Anderson-Darling Test** (For large datasets)

* Use when data has **more than 5000 rows**
* It compares how well your data fits a normal distribution (or other distributions)

---

### ğŸ“ˆ 3. **QQ Plot (Quantile-Quantile Plot)**

This is a **visual check** for normality.

* Each point represents a value in your dataset
* If your data is normally distributed â¡ï¸ **The points will lie on a straight diagonal line**

ğŸ–¼ï¸ Visual Example:
![image](https://github.com/user-attachments/assets/78e92282-c4fc-40af-895e-486a6c686436)

âœ… Straight line = Data looks normal
âŒ Curved or scattered = Data is **not normal**

---

### ğŸ§  Summary Table:

| Method               | Use When...              | Interpretation                       |
| -------------------- | ------------------------ | ------------------------------------ |
| **Shapiro-Wilk**     | Data size â‰¤ 5000         | p > 0.05 â†’ Data is normal            |
| **Anderson-Darling** | Data size > 5000         | Compare statistic to critical values |
| **QQ Plot**          | Any size (visual method) | Straight line = Normal distribution  |

---

## ğŸ“š The Central Limit Theorem (CLT) ğŸ”

### ğŸ§ Problem:

What if your **population data is not normally distributed**?
It could be **left-skewed**, **right-skewed**, or just irregular!

ğŸ“Œ Example:
Imagine you have exam marks of **10 lakh students**, and the distribution is **skewed**.

---

### ğŸ§  What CLT Says:

Even if the **population is not normal**, the **distribution of sample means** will be **normal** â€” if the sample size is large enough.

ğŸ¯ **Thatâ€™s the Central Limit Theorem!**

---

### ğŸ”¬ Step-by-Step:

1. Start with a **non-normal population**
   ğŸ‘‰ e.g., 10 lakh studentsâ€™ marks ğŸ§‘â€ğŸ“

2. Take **multiple random samples** from this population
   ğŸ‘‰ e.g., 500 samples, each of size 100 students

3. For each sample, calculate the **average** (called **sample mean**, or xÌ„)

4. Plot these **sample means** on a graph ğŸ“ˆ

5. âœ… Result: You will get a **normal distribution**, even if the original data wasnâ€™t normal!

---

### ğŸ§® Requirements:

* ğŸ“ **Minimum sample size (n): 30** is generally considered enough for CLT to kick in
* ğŸ§º **Number of samples:** Thereâ€™s **no fixed limit**, more samples = better approximation

---

### âœ… Why CLT is Important:

* Allows you to use **normal distribution techniques** (like Z-scores, confidence intervals, hypothesis tests) **even when your population isnâ€™t normal**
* **Simplifies analysis** of real-world data, which often isn't clean or normal

---

### ğŸ“Œ Final Takeaway:

CLT lets you **treat the population as normally distributed** when working with large enough random samplesâ€”even if it clearly isnâ€™t!

---

## ğŸ” Central Limit Theorem (CLT)

### ğŸ“‰ Problem:

What if your **population data is NOT normally distributed**?
It could be:

* Left-skewed (tail on the left)
* Right-skewed (tail on the right)
* Uneven or irregular

Example: You have exam marks of **10 lakh students**, and the data is skewed.

---

### ğŸ§  What CLT Tells Us:

Even if the **original population is skewed**, the **distribution of sample means** becomes **approximately normal** when:

* Samples are **random**
* Sample size is **large enough** (typically **n â‰¥ 30**)

---

### ğŸ”¬ How it Works (Step-by-step):

1. ğŸ“ Take multiple **random samples** from the population
   ğŸ‘‰ Example: 500 samples, each of 100 students from 10 lakh students

2. âœï¸ For each sample, calculate the **sample mean (xÌ„)**

3. ğŸ“ˆ Plot all the sample means on a graph

4. ğŸ¯ **Surprise!** The plot forms a **bell-shaped normal distribution**

---

### ğŸ“ Key Points:

| Concept           | Meaning                                                             |
| ----------------- | ------------------------------------------------------------------- |
| Sample Mean (xÌ„)  | Average of each sample                                              |
| Sample Size (n)   | Ideally **â‰¥ 30** for CLT to apply                                   |
| Number of Samples | No fixed number â€” **more is better** for smoother bell curve        |
| Result            | Sample means form a **normal distribution**, even if original isn't |
| Conclusion        | You can treat your data as **normally distributed** for analysis    |

---

### ğŸ“Š Why CLT Matters:

It lets you apply powerful statistical tools (like **Z-scores**, **confidence intervals**, **hypothesis tests**) **even when the raw data isnâ€™t normal**.

This is **super useful in real-world data science**, where raw data is often messy!

---

### ğŸ–¼ï¸ Visual Representation:

![image](https://github.com/user-attachments/assets/e696b9e6-487e-436c-82c1-8718e8ac87a4)

You can see:

* Population data (skewed)
* Multiple sample means
* Normal curve emerging from those sample means!

---

## ğŸ§ª Hypothesis Testing: Understanding the Basics

### ğŸ” Why Hypothesis Testing?

* Itâ€™s **impossible to test an entire population** (too big, costly, or time-consuming)
* So, we **test a sample** and **make conclusions about the population**
  This process is called **Inferential Statistics**

---

### ğŸ“ Steps in Hypothesis Testing

1. **State the Assumptions**

   * **Null Hypothesis (Hâ‚€):** The default assumption (e.g., no effect, no difference)
   * **Alternate Hypothesis (Hâ‚ or Ha):** What you want to test or prove

2. **Decide the Level of Significance (Î±)**

   * This is the **probability of making a mistake** (Type I error: rejecting a true null)
   * Commonly used values: **0.05 (5%)**, 0.01 (1%), or 0.10 (10%)

3. **Calculate the Test Statistic**

   * A number summarizing how much your sample data deviates from the null hypothesis
   * Examples: Z-score, t-score, Chi-square statistic depending on the test

4. **Find the p-value**

   * The **probability** of observing your sample data (or more extreme) assuming the null hypothesis is true
   * Low p-value means the observed data is unlikely under Hâ‚€

5. **Make a Decision**

   * If **p-value < Î±** â†’ Reject the Null Hypothesis â†’ There is evidence supporting the alternate hypothesis
   * If **p-value â‰¥ Î±** â†’ Fail to reject Null Hypothesis â†’ Not enough evidence to reject Hâ‚€

---

### âš–ï¸ Alternative Decision Rule

* Instead of p-value, compare **Test Statistic** with **Critical Value** (from statistical tables)
* If Test Statistic > Critical Value â†’ Reject Hâ‚€
* Else â†’ Fail to reject Hâ‚€

---

### ğŸ“Œ Key Terms

| Term                          | Meaning                                 |
| ----------------------------- | --------------------------------------- |
| **Sample Statistic** (xÌ„)     | Average of the sample data              |
| **Population Parameter** (Î¼)  | True average of the entire population   |
| **Î± (Level of Significance)** | Threshold probability for Type I error  |
| **p-value**                   | Probability that supports Hâ‚€ given data |

---

### ğŸ’¡ Summary:

Hypothesis testing helps you **decide whether the data you have supports your assumption about the population**â€”all based on the sample you tested!

---

## âš–ï¸ Null and Alternate Hypotheses

### ğŸ§© What are they?

* **Null Hypothesis (Hâ‚€):**
  The **neutral** or **status quo** statement
  â€” assumes **no effect** or **no change**
  â€” usually has **=, â‰¤, or â‰¥** sign

* **Alternate Hypothesis (Hâ‚ or Ha):**
  What we **actually want to test or prove**
  â€” assumes there **is an effect or change**

---

### ğŸ¯ Example:

Suppose, **40% of students get placed before doing a C-DAC course**.

* **Null Hypothesis (Hâ‚€):**
  No change in placement chances after C-DAC course
  â†’ $\mu = 40\%$ (or $\mu \leq 40\%$, $\mu \geq 40\%$)

* **Alternate Hypothesis (Hâ‚):**
  There **is a significant difference** in placement chances

  * If checking for **any change**: $\mu \neq 40\%$ (two-sided test)
  * If checking for **a decrease**: $\mu < 40\%$ (one-sided test)
  * If checking for **an improvement**: $\mu > 40\%$ (one-sided test)

---

### â— Important Note:

We **never prove or accept** a hypothesis!
We can only:

* **Reject the Null Hypothesis** (meaning data shows strong evidence for the alternate)
* Or **Fail to Reject the Null Hypothesis** (meaning data is insufficient to conclude a change)

---

### â“ Why donâ€™t we â€œacceptâ€ hypotheses?

* Because **statistics never prove something with 100% certainty** â€” it only tells us how likely or unlikely our observed data is under an assumption
* So, we only say:

  * "Reject Hâ‚€" â†’ strong evidence against Hâ‚€
  * "Fail to reject Hâ‚€" â†’ not enough evidence to reject Hâ‚€ (but it might still be false)

---

## ğŸ¯ Significance Level (Î±) â€” What Is It and Why Do We Need It?

### ğŸ” Problem Setup:

* **Null Hypothesis (Hâ‚€):** No change in placement chances after C-DAC course
  â†’ $\mu = 40\%$

* You collect sample data, calculate sample mean $\bar{x}$, and want to decide:

  * If $\bar{x} = 38\%$, should you reject or accept Hâ‚€?
  * If $\bar{x} = 75\%$, should you reject or accept Hâ‚€?

### â“ How do we decide if the sample mean is "close enough" to population mean?

* Is 38 close to 40?
* Is 75 close to 40?

You **cannot just rely on your intuition** here. We need a **statistical rule** or **proof** to decide!

---

## âš¡ Enter: Significance Level (Alpha, $\alpha$)

* Alpha $\alpha$ = the **probability (risk) of rejecting the null hypothesis even though it is actually true**
* Also called the **"Rejection Zone"** or **"Area of Rejection"**

---

### ğŸ§ Why do we need Alpha?

Imagine this:

* $\mu = 40$, but your sample mean is $\bar{x} = 75$
* This looks very different, so you might want to reject Hâ‚€

But what if you **reject Hâ‚€ wrongly?**
Think of tossing a coin 5 times (Hâ‚€: coin is fair).
You get heads all 5 times (HHHHH).
Would you say the coin is unfair? Maybe â€” but there's still a small chance this happened by pure luck!

Rejecting Hâ‚€ when itâ€™s actually true = **Type I error**
Alpha ($\alpha$) controls how much risk you are willing to take for this mistake

---

### ğŸ² Example of Alpha in Action

* Suppose:
  $\mu = 40$, sample mean $\bar{x} = 75$, standard deviation $s = 3$, sample size $n = 50$

* Calculate Z-Score:

  $$
  Z = \frac{\bar{x} - \mu}{s / \sqrt{n}} = \frac{75 - 40}{3 / \sqrt{50}} = \frac{35}{0.424} \approx 82.5
  $$

* This Z is extremely high (very far from mean), so clearly we would **reject Hâ‚€** here.

---

### ğŸ¯ How to decide when to reject or not reject Hâ‚€?

* If Alpha $\alpha = 0$, **we never reject Hâ‚€** (too strict!)
* If Alpha $\alpha = 100\%$, **we always reject Hâ‚€** (too lenient!)

So, a good balance is needed â€” usually **$\alpha = 0.05$ (5%)** is chosen

---

### ğŸ”¥ What does $\alpha = 0.05$ mean?

* We **reject Hâ‚€** only if our sample data falls in the **extreme 5% of values** (called the rejection region)
* For a **two-tailed test**, this 5% is split equally as **2.5% in each tail** of the distribution

---

### ğŸ“ What is the Z-Score boundary for rejection at $\alpha = 0.05$?

* The critical Z-scores at the boundary of the rejection zone are:

  $$
  \pm 1.96
  $$

* Why 1.96?
  Because **95% of data lies between -1.96 and +1.96** in a standard normal distribution, leaving **2.5% on each tail**

---

### ğŸ§  So, the decision rule with $\alpha = 0.05$ is:

* If **Z-score < -1.96** or **Z-score > 1.96**, **Reject Null Hypothesis (Hâ‚€)**
* Else, **Fail to Reject Null Hypothesis**

---

## âš–ï¸ Type I and Type II Errors, Alpha (Î±) and Beta (Î²)

|                      | **Hâ‚€ is True**       | **Hâ‚€ is False**       |
| -------------------- | -------------------- | --------------------- |
| **Reject Hâ‚€**        | **Type I Error (Î±)** | **Correct Decision**  |
| **Do Not Reject Hâ‚€** | **Correct Decision** | **Type II Error (Î²)** |

---

### What does this mean? ğŸ¤”

* **Type I Error (Î±)**:
  You **reject the null hypothesis (Hâ‚€) when it is actually true**
  â†’ False alarm!
  Example: You think the placement chances changed after C-DAC, but they actually did not.

* **Type II Error (Î²)**:
  You **fail to reject the null hypothesis when it is actually false**
  â†’ Missed detection!
  Example: You conclude no change in placement chances after C-DAC, but actually chances improved.

---

### Why do these errors happen? ğŸ¤·â€â™‚ï¸

* Because of **bad or limited sample data** that does not perfectly represent the whole population
* For example:
  You get a sample average score = 80 (higher than population mean), so you reject Hâ‚€, but this was just a **lucky sample** â€” Hâ‚€ might still be true when considering all data.

---

### Trade-off between Type I and Type II errors âš–ï¸

* If you **try to reduce Type I errors (false alarms)** by setting a very low alpha (Î±), you become very strict about rejecting Hâ‚€.
* But, this **increases the chance of Type II errors (missed detections)** â€” you might fail to detect real effects.
* Conversely, if you try to reduce Type II errors, Type I errors may increase.

---

### Summary:

* **Alpha (Î±)** = probability of Type I error (rejecting Hâ‚€ when true)
* **Beta (Î²)** = probability of Type II error (not rejecting Hâ‚€ when false)
* Balancing these errors is important depending on the problem context

---

Hereâ€™s a clear and simple breakdown of your example with some emojis for easier understanding:

---

## ğŸš— Type I and Type II Errors â€“ Real-Life Example

**Null Hypothesis (Hâ‚€):**
Johnâ€™s used car **is safe** to drive.

---

### Possible Situations:

| Situation                                                        | Error Type / Outcome               |
| ---------------------------------------------------------------- | ---------------------------------- |
| (a) John thinks car is **safe**, but it is actually **not safe** | **Type II Error (False Negative)** |
| (b) John thinks car is **safe**, and it **is safe**              | **Correct decision**               |
| (c) John thinks car is **not safe**, and it **is not safe**      | **Correct decision**               |
| (d) John thinks car is **not safe**, but it **is safe**          | **Type I Error (False Positive)**  |

---

### Explanation:

* **Type I Error (Î±):** Rejecting Hâ‚€ when Hâ‚€ is true.
  Here, it means John **thinks the car is not safe when it actually is safe** (situation d).

* **Type II Error (Î²):** Failing to reject Hâ‚€ when Hâ‚€ is false.
  Here, it means John **thinks the car is safe when it actually is not safe** (situation a).

---

### Which error is more serious? âš ï¸

* **Type II Error** is more serious here:
  John believes the car is safe when it is actually **not safe** â€” this can lead to accidents and danger.

* Type I Error means unnecessary caution (thinking car is unsafe when it is safe), which is less harmful in this context.

---

## ğŸš— Type I and Type II Errors â€“ Real-Life Example

**Null Hypothesis (Hâ‚€):**
Johnâ€™s used car **is safe** to drive.

---

### Possible Situations:

| Situation                                                        | Error Type / Outcome               |
| ---------------------------------------------------------------- | ---------------------------------- |
| (a) John thinks car is **safe**, but it is actually **not safe** | **Type II Error (False Negative)** |
| (b) John thinks car is **safe**, and it **is safe**              | **Correct decision**               |
| (c) John thinks car is **not safe**, and it **is not safe**      | **Correct decision**               |
| (d) John thinks car is **not safe**, but it **is safe**          | **Type I Error (False Positive)**  |

---

### Explanation:

* **Type I Error (Î±):** Rejecting Hâ‚€ when Hâ‚€ is true.
  Here, it means John **thinks the car is not safe when it actually is safe** (situation d).

* **Type II Error (Î²):** Failing to reject Hâ‚€ when Hâ‚€ is false.
  Here, it means John **thinks the car is safe when it actually is not safe** (situation a).

---

### Which error is more serious? âš ï¸

* **Type II Error** is more serious here:
  John believes the car is safe when it is actually **not safe** â€” this can lead to accidents and danger.

* Type I Error means unnecessary caution (thinking car is unsafe when it is safe), which is less harmful in this context.

---

## âš–ï¸ Type I and Type II Errors â€“ Criminal Court Case Example

**Null Hypothesis (Hâ‚€):**
The defendant **is innocent**.

---

### Possible Scenarios:

| Situation                                                           | Error Type / Outcome               |
| ------------------------------------------------------------------- | ---------------------------------- |
| (a) Jury believes defendant is **guilty** but she is **innocent**   | **Type I Error (False Positive)**  |
| (b) Jury believes defendant is **guilty** and she **is guilty**     | **Correct decision**               |
| (c) Jury believes defendant is **innocent** but she **is guilty**   | **Type II Error (False Negative)** |
| (d) Jury believes defendant is **innocent** and she **is innocent** | **Correct decision**               |

---

### Explanation:

* **Type I Error (Î±):** Rejecting Hâ‚€ when it is actually true.
  Jury wrongly convicts an innocent person (situation a).

* **Type II Error (Î²):** Failing to reject Hâ‚€ when it is actually false.
  Jury lets a guilty person go free (situation c).

---

### Which error is more serious? âš–ï¸

* This depends on the context and perspective:

  * **Type I Error (wrongly convicting the innocent)** can ruin innocent lives.
  * **Type II Error (letting guilty go free)** can be dangerous for society.

* In many legal systems, **Type I Error** is considered more serious because *"It is better that ten guilty persons escape than that one innocent suffer."*
now if you want me to simplify or add examples from other fields too!

---

Type 1 error:
Rejecting a null hypothesis when we should not

Type 2 error:
Not rejecting null hypothesis when you should have
