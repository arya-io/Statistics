# ğŸ§ª One-Tailed vs. Two-Tailed Tests

## ğŸ¯ What is a "Tail" in Statistics?

A **tail** refers to the ends of a probability distribution curve. These are the areas where **extreme values** fall.
In hypothesis testing, **tails** represent the **rejection regions**â€”places where the results are so unusual that we reject the **Null Hypothesis**.

> âœ… **Rejection Region** = Where your test result must fall to reject the Null Hypothesis.
> ğŸ”º This region is also where **alpha (Î±)** livesâ€”the significance level.

---

## ğŸ” Two-Tailed Test

### ğŸ“Œ Key Points:

* **Rejection region** is on **both sides** of the curve.
* **Alpha (Î±)** is **split into two halves** â†’ we use **Î±/2** on each tail.
* Used when we are testing for **any difference** (not specific direction).

### ğŸ§  Think of it like:

> "I don't know if it's higher or lowerâ€”I just want to know if it's **different**."

### ğŸ“ Hypotheses:

* **Null Hypothesis (Hâ‚€)**: = (equal to)
* **Alternative Hypothesis (Hâ‚)**: â‰  (not equal to)

### ğŸ” Example:

> You want to test if a new teaching method has a different effect on student scores (could be higher or lower). Since youâ€™re open to both possibilities, itâ€™s a **two-tailed test**.

---

## ğŸ‘‰ One-Tailed Test

### ğŸ“Œ Key Points:

* Rejection region is on **one side** of the curve only.
* The **entire Î±** is in **one tail**.
* Used when you are testing in **one direction** only.

### ğŸ§  Think of it like:

> "I believe this new medicine will work **better** (or **worse**) than the old one."

### ğŸ“ Hypotheses:

* **Null Hypothesis (Hâ‚€)**: = (equal to)
* **Alternative Hypothesis (Hâ‚)**: > (greater than) or < (less than)

> âš ï¸ Be careful: Use a one-tailed test only when you're **confident** the effect is in one direction.

### ğŸ” Example:

> You want to check if a new battery lasts **longer** than the old one (not shorter). This is a **one-tailed test** (right tail).

---

âœ… **Quick Recap Table:**

| Feature              | Two-Tailed Test          | One-Tailed Test                   |
| -------------------- | ------------------------ | --------------------------------- |
| **Rejection Region** | Both sides               | One side only                     |
| **Alpha Used**       | Î±/2 on each side         | Full Î± on one side                |
| **Hâ‚€**               | =                        | =                                 |
| **Hâ‚**               | â‰                         | > or <                            |
| **Use When**         | Any difference (up/down) | Specific direction (up *or* down) |

---

# ğŸ§ª One-Tailed vs. Two-Tailed Tests â€“ Explained Simply!

---

## ğŸ§¾ Writing Hypotheses (Hâ‚€ & Hâ‚) in Different Ways

| **Hâ‚€ (Null Hypothesis)** | **Hâ‚ (Alternative Hypothesis)** | **Type of Test**   |
| ------------------------ | ------------------------------- | ------------------ |
| =                        | â‰                                | Two-tailed Test ğŸ” |
| â‰¤                        | >                               | One-tailed Test ğŸ‘‰ |
| â‰¥                        | <                               | One-tailed Test ğŸ‘ˆ |

---

## ğŸ“ Real-World Examples

### ğŸ” **Two-Tailed Test Example**

> ğŸ“¢ A battery company claims its batteries last **20 hours** on average.

* **Hâ‚€:** Î¼ = 20
* **Hâ‚:** Î¼ â‰  20
  ğŸ’¡ You're checking if the battery life is **different** (could be more or less).

---

### ğŸ‘‰ **One-Tailed Test Example**

> ğŸ’Š A company claims its new medicine **reduces** fever duration from 16 weeks.

* **Hâ‚€:** Î¼ â‰¥ 16
* **Hâ‚:** Î¼ < 16
  ğŸ’¡ Youâ€™re only interested if the medicine makes it **less**, not more.

---

## ğŸ” Two-Tailed Test ğŸ§ª

### ğŸ”¹ Key Features:

* Uses **=** and **â‰ ** in Hâ‚€ and Hâ‚.
* We're **not predicting a direction** (not saying greater or lesser).
* Just testing **if there's a difference**â€”in **either** direction.
* So the **rejection region** is on **both sides** (tails) of the distribution.
* We use **Î±/2** on each tail.

### ğŸ§  Think of it like:

> â€œI donâ€™t know if itâ€™s better or worseâ€”I just want to know if itâ€™s **different**.â€

### ğŸ“ Conclusion Rule:

> âŒ Reject Hâ‚€ if:

* Test statistic < -Z (at Î±/2)
  **OR**
* Test statistic > +Z (at Î±/2)

---

## ğŸ‘‰ One-Tailed Test ğŸ§ª

### ğŸ”¹ Key Features:

* Uses **â‰¤ / >** or **â‰¥ / <** in Hâ‚€ and Hâ‚.
* Predicts a **specific direction** of difference.
* Rejection region is only on **one side** of the distribution.
* Uses **entire Î±** in one tail.

### ğŸ“Œ Clue:

> If the statement says **"reduces"** or **"improves"**, it's most likely **one-tailed**.
> If it says **"changes"** or just **"different"**, it's a **two-tailed** test.

### ğŸ“ Conclusion Rule:

> âŒ Reject Hâ‚€ if the test statistic falls into the **one** rejection region (left or right tail depending on direction).

---

## ğŸ¯ Summary Table: One-Tailed vs. Two-Tailed

| Feature                | Two-Tailed Test ğŸ”    | One-Tailed Test ğŸ‘‰ / ğŸ‘ˆ        |
| ---------------------- | --------------------- | ------------------------------ |
| Rejection Region       | Both sides            | One side only                  |
| Alpha (Î±) Distribution | Î±/2 on each side      | All Î± in one side              |
| Hypothesis Symbols     | = vs â‰                 | â‰¤ vs > or â‰¥ vs <               |
| Direction Predicted?   | No (just different)   | Yes (greater or less)          |
| Real-life Clue Words   | "change", "different" | "increase", "reduce", "better" |

---

## ğŸ” Extra Example: Lifespan of a Medicine

> Current average lifespan = **70 years**

### âœ… Two-Tailed Test:

* **Hâ‚€:** Î¼ = 70
* **Hâ‚:** Î¼ â‰  70
  ğŸ’¬ We're just checking if it's **different**, not caring if it's more or less.

### âœ… One-Tailed Test:

* **Hâ‚€:** Î¼ = 70
* **Hâ‚:** Î¼ > 70
  ğŸ’¬ We're checking if the **new medicine increases lifespan**.

---

# ğŸŸ Hypothesis Testing with Chips â€“ One-Tailed & Two-Tailed Tests

Letâ€™s understand hypothesis testing using a **chips packet** ğŸ¥” as an example.

---

## ğŸ” Two-Tailed Test â€“ When We're Just Checking for Any Difference

### ğŸ” Scenario:

> A chips company claims each packet contains **exactly 50 grams** of chips. You want to **verify** this claim.

### ğŸ§¾ Hypotheses:

* **Hâ‚€ (Null):** Î¼ = 50
* **Hâ‚ (Alternate):** Î¼ â‰  50

ğŸ“Š **Rejection Zone:**

* Lies on **both sides** of the mean (left and right of 50 grams).
* You're testing if the weight is **either less than** or **more than** 50 grams.

ğŸ’¬ Think of it like:

> "I donâ€™t care if the packet is heavier or lighterâ€”if itâ€™s not **exactly** 50 grams, Iâ€™m concerned."

---

## ğŸ‘ˆ One-Tailed Test â€“ Left-Tailed (Checking if Itâ€™s Less)

### ğŸ” Scenario:

> You suspect the chips company is giving **less than 50 grams** of chips.

### ğŸ§¾ Hypotheses:

* **Hâ‚€ (Null):** Î¼ â‰¥ 50
* **Hâ‚ (Alternate):** Î¼ < 50

ğŸ“Š **Rejection Zone:**

* Lies on the **left side** of the distribution (less than 50 grams).

ğŸ’¬ Think of it like:

> "I think the packets are **underfilled**."

---

## ğŸ‘‰ One-Tailed Test â€“ Right-Tailed (Checking if Itâ€™s More)

### ğŸ” Scenario:

> You're checking if the company is generously giving **more than 50 grams** of chips per packet.

### ğŸ§¾ Hypotheses:

* **Hâ‚€ (Null):** Î¼ â‰¤ 50
* **Hâ‚ (Alternate):** Î¼ > 50

ğŸ“Š **Rejection Zone:**

* Lies on the **right side** of the distribution (more than 50 grams).

ğŸ’¬ Think of it like:

> "Letâ€™s test if theyâ€™re being extra generous!" ğŸ˜„

---

## âœ… Quick Visual Recap:

| **Test Type**         | **Hâ‚€** | **Hâ‚** | **Rejection Zone**        |
| --------------------- | ------ | ------ | ------------------------- |
| ğŸ” Two-Tailed         | Î¼ = 50 | Î¼ â‰  50 | Both left and right tails |
| ğŸ‘ˆ One-Tailed (Left)  | Î¼ â‰¥ 50 | Î¼ < 50 | Left tail (less than 50)  |
| ğŸ‘‰ One-Tailed (Right) | Î¼ â‰¤ 50 | Î¼ > 50 | Right tail (more than 50) |

---

![image](https://github.com/user-attachments/assets/6590ae45-931d-4425-a5e9-527134bec511)

![image](https://github.com/user-attachments/assets/4d000a65-448d-4d15-bde8-3449e895a3dc)

![image](https://github.com/user-attachments/assets/c059ecd1-00d5-470e-b726-15ca9c3ed405)

---

# ğŸ‘‰ One-Tailed Tests â€“ Explained Simply!

---

## ğŸ“Œ What is a One-Tailed Test?

A **One-Tailed Test** checks if the sample data is significantly **greater than** or **less than** a certain valueâ€”not just different.

ğŸ’¡ It focuses on **one side (tail)** of the probability distribution.

Thatâ€™s why it's called a **"one-tailed" test** â€” because the **rejection region** (where we might reject the Null Hypothesis) is only on **one tail** of the curve.

---

## ğŸ“Š How the Hypotheses Are Written

In one-tailed tests, the **Null Hypothesis (Hâ‚€)** always includes:

* **â‰¤ (less than or equal to)**
  or
* **â‰¥ (greater than or equal to)**

The **Alternative Hypothesis (Hâ‚)** uses:

* **> (greater than)**
  or
* **< (less than)**

---

## â— Rejection Region

* ğŸ”´ The **rejection region** is concentrated on **only one side** of the distribution.
* All of the **significance level (Î±)** lies in that **one tail**.

> ğŸ§  Tip: If you use a **one-tailed test**, **do not divide Î± by 2**.

---

## ğŸ§­ Two Types of One-Tailed Tests

| **Test Type**            | **Used When**                              | **Symbol**    | **Rejection Tail**      |
| ------------------------ | ------------------------------------------ | ------------- | ----------------------- |
| ğŸ‘ˆ **Left-Tailed Test**  | You're testing if something is **less**    | Hâ‚: Î¼ < value | Left side (lower tail)  |
| ğŸ‘‰ **Right-Tailed Test** | You're testing if something is **greater** | Hâ‚: Î¼ > value | Right side (upper tail) |

---

### ğŸ§ª Example Scenarios

* **Left-tailed test:**
  A company wants to know if the **actual product weight is less** than the advertised 50 grams.

  * Hâ‚€: Î¼ â‰¥ 50
  * Hâ‚: Î¼ < 50
    â¡ï¸ Reject Hâ‚€ if the test statistic falls far enough **left**.

* **Right-tailed test:**
  A drug company wants to prove that its medicine **increases** recovery speed (e.g., recovery time is faster than 10 days).

  * Hâ‚€: Î¼ â‰¤ 10
  * Hâ‚: Î¼ > 10
    â¡ï¸ Reject Hâ‚€ if the test statistic falls far enough **right**.

---

ğŸ§  **Remember:**

* Use **one-tailed** only when you're sure about the **direction** of the effect.
* If you're just testing for **any difference (not direction)**, use a **two-tailed test** instead.

---

# ğŸ¯ One-Tailed Tests â€“ Left vs Right Tail

When you're **sure of the direction** you expect the result to go in (either less than or more than a value), you use a **one-tailed test**. Letâ€™s break it into two types with examples and reasoning ğŸ‘‡

---

## ğŸ‘ˆ LEFT-TAILED TEST

â¡ï¸ **Rejection Region: Left Side**

---

### ğŸ’Š **Example Scenario: Medicine Reducing Fever**

> A new medicine is being tested. Normally, fever lasts **16 weeks**. We want to check if the medicine **reduces** this duration.

### ğŸ§¾ Hypotheses:

* **Hâ‚€ (Null):** Î¼ â‰¥ 16 (fever lasts 16 weeks or more â†’ medicine has no effect)
* **Hâ‚ (Alternate):** Î¼ < 16 (fever lasts less than 16 weeks â†’ medicine is effective)

---

### ğŸ“‰ Why is Rejection on the **Left**?

* If the **test statistic** (z-score) is **greater than or equal to 16**, that supports Hâ‚€ â†’ so we **do not reject** it.
* But if the **test statistic is much less than 16**, this means the medicine is **actually reducing** fever duration â†’ so we **reject Hâ‚€**.

---

### ğŸ§® What About the Z-Score?

**Z = (xÌ„ - Î¼) / (s / âˆšn)**

* If the sample mean xÌ„ is **less than** Î¼ (16), the numerator becomes negative â†’ **z-score is negative**
* This puts the test statistic in the **left tail** of the distribution

âœ… **So we reject Hâ‚€ if:**
**Z < Z-critical** (at chosen Î± level)

---

## ğŸ‘‰ RIGHT-TAILED TEST

â¡ï¸ **Rejection Region: Right Side**

---

### ğŸ”‹ **Example Scenario: Battery Life with New Technology**

> A battery that used to last **30 hours** is now claimed to last **more** due to a new technology.

### ğŸ§¾ Hypotheses:

* **Hâ‚€ (Null):** Î¼ â‰¤ 30 (no improvement in battery life)
* **Hâ‚ (Alternate):** Î¼ > 30 (battery life has increased)

---

### ğŸ“ˆ Why is Rejection on the **Right**?

* If the **test statistic** is **30 or less**, that supports Hâ‚€ â†’ new tech is not improving battery life â†’ **do not reject Hâ‚€**
* If the **test statistic is much greater than 30**, it supports Hâ‚ â†’ **reject Hâ‚€**

---

### ğŸ§® What About the Z-Score?

**Z = (xÌ„ - Î¼) / (s / âˆšn)**

* If xÌ„ is **greater than** Î¼ (30), numerator is positive â†’ **z-score is positive**
* That means the test statistic falls in the **right tail**

âœ… **So we reject Hâ‚€ if:**
**Z > Z-critical** (at Î±)

---

## ğŸ“Š Quick Summary Table

| **Test Type**        | **Hâ‚€**    | **Hâ‚**    | **Rejection Zone** | **Z-Score Direction** |
| -------------------- | --------- | --------- | ------------------ | --------------------- |
| ğŸ‘ˆ Left-Tailed Test  | Î¼ â‰¥ value | Î¼ < value | Left side          | Negative              |
| ğŸ‘‰ Right-Tailed Test | Î¼ â‰¤ value | Î¼ > value | Right side         | Positive              |

---

### ğŸ”‘ Key Formula:

**Z = (Sample Mean - Population Mean) / (Standard Deviation / âˆšSample Size)**

---

# ğŸ“ Hypothesis Testing: C-DAC Placement Example

> ğŸ“Š Suppose **the chance of a student getting placed before doing a C-DAC course is 40% (Î¼ = 40%)**.
> Letâ€™s test different hypotheses using one-tailed and two-tailed approaches.

---

## 1ï¸âƒ£ ğŸ” **Two-Tailed Test** â€“ Just Checking for Any Difference

### âœï¸ Hypotheses:

* **Hâ‚€ (Null):** Î¼ = 40%
* **Hâ‚ (Alternate):** Î¼ â‰  40%

### â— When to Use:

> You're **not sure** if the C-DAC course increases or decreases placement chancesâ€”you just want to know if it has **any impact**.

### ğŸš© Rejection Zones:

* ğŸ”» **Two regions**: one in the **left tail** and one in the **right tail**
* Each region contains: **Î± / 2**

For example: If Î± = 0.05
â†’ Each tail gets **0.025** rejection zone

---

## 2ï¸âƒ£ ğŸ‘ˆ **Left-Tailed Test** â€“ Checking If It's Worse

### âœï¸ Hypotheses:

* **Hâ‚€ (Null):** Î¼ â‰¥ 40%
* **Hâ‚ (Alternate):** Î¼ < 40%

### â— When to Use:

> You suspect that students have a **lower chance** of getting placed before doing C-DAC.

### ğŸš© Rejection Zone:

* ğŸ”» **Left tail** only
* Entire **Î± value is on the left**

Example: Î± = 0.05
â†’ **All 0.05 is in the left rejection region**

---

## 3ï¸âƒ£ ğŸ‘‰ **Right-Tailed Test** â€“ Checking If It's Better

### âœï¸ Hypotheses:

* **Hâ‚€ (Null):** Î¼ â‰¤ 40%
* **Hâ‚ (Alternate):** Î¼ > 40%

### â— When to Use:

> You believe the C-DAC course **increases** placement chances.

### ğŸš© Rejection Zone:

* ğŸ”º **Right tail** only
* Entire **Î± value is on the right**

Example: Î± = 0.05
â†’ **All 0.05 is in the right rejection region**

---

## ğŸ§  Visual Summary

| **Test Type**   | **Hâ‚€**  | **Hâ‚**  | **Rejection Region(s)** | **Alpha (Î±) Location** |
| --------------- | ------- | ------- | ----------------------- | ---------------------- |
| ğŸ” Two-Tailed   | Î¼ = 40% | Î¼ â‰  40% | Left and Right tails    | Î±/2 in each tail       |
| ğŸ‘ˆ Left-Tailed  | Î¼ â‰¥ 40% | Î¼ < 40% | Left tail only          | Full Î± on left         |
| ğŸ‘‰ Right-Tailed | Î¼ â‰¤ 40% | Î¼ > 40% | Right tail only         | Full Î± on right        |

---

# ğŸ§ª Impact of Test Type on Alpha (Î±)

---

When we perform hypothesis testing, we choose a **significance level (Î±)** â€” commonly **0.05 or 5%** â€” which defines how much risk we're willing to take of making a **Type I Error** (i.e., rejecting a true null hypothesis).

But depending on the type of test (**one-tailed or two-tailed**), **Î± is handled differently**. Let's break it down ğŸ‘‡

---

## ğŸ” Two-Tailed Test

> We are testing if the value is **simply different** (either **higher or lower**) from the hypothesized value.

### ğŸ“Œ What happens to Î±?

* There are **two rejection regions**:

  * One in the **lower tail**
  * One in the **upper tail**
* The total Î± is **split equally** into both tails.

### âœ… Example:

If **Î± = 0.05**:

* **0.025 (2.5%)** goes into the **left tail**
* **0.025 (2.5%)** goes into the **right tail**

ğŸ¯ So, you **reject Hâ‚€** if your test statistic falls:

* **Below the lower 2.5%**
* **Above the upper 2.5%**

---

## ğŸ‘‰ğŸ‘ˆ One-Tailed Test (Left or Right)

> We are checking if the value is **only greater than** or **only less than** the hypothesized value.

### ğŸ“Œ What happens to Î±?

* There is **only one rejection region**:

  * **Left-tailed**: Î± goes to the **lower end**
  * **Right-tailed**: Î± goes to the **upper end**
* The **entire Î±** is concentrated in **one tail only**

### âœ… Example:

If **Î± = 0.05**:

* All **0.05 (5%)** goes into **just one tail**, depending on the direction.

ğŸ¯ So, you **reject Hâ‚€** if your test statistic:

* Falls in the **extreme left** (for left-tailed)
* Falls in the **extreme right** (for right-tailed)

---

## ğŸ§  Visual Summary Table

| **Test Type**        | **No. of Rejection Regions** | **Alpha Distribution**               |
| -------------------- | ---------------------------- | ------------------------------------ |
| ğŸ” Two-Tailed Test   | 2 (both tails)               | Î±/2 in left tail + Î±/2 in right tail |
| ğŸ‘ˆ Left-Tailed Test  | 1 (left tail)                | Entire Î± in left tail                |
| ğŸ‘‰ Right-Tailed Test | 1 (right tail)               | Entire Î± in right tail               |

---

## ğŸ“ Quick Recap:

* âœ‚ï¸ **Split Î±** in two-tailed tests
* âœ… **Do not split Î±** in one-tailed tests

---

# ğŸ§ª Two-Tailed Tests: Understanding the p-Value ğŸ¯

In hypothesis testing, the **p-value** tells us the probability of getting a result **as extreme as** the one observed, assuming the **null hypothesis is true**.

In a **two-tailed test**, we're looking for evidence in **both directions** (less than or greater than), so we have to **account for both tails** of the distribution.

---

## ğŸ”¢ Step-by-Step: How to Calculate p-Value in a Two-Tailed Test

---

### 1ï¸âƒ£ Calculate the Z-Test Statistic ğŸ§®

Use the formula:

$$
Z = \frac{\bar{x} - \mu}{s / \sqrt{n}}
$$

This Z value could be **positive or negative**, depending on whether the sample mean is above or below the population mean.

---

### 2ï¸âƒ£ Find Area to the **Left of Z** in Z-Table ğŸ“˜

Look up the calculated Z in the Z-table.
ğŸ‘‰ It gives you the **area under the curve to the left** of the Z value.

### âœ… Example:

Letâ€™s say **Z = 1.23**
From the **Z-table**, the area to the **left** is:

$$
P(Z < 1.23) = 0.8907
$$

---

### 3ï¸âƒ£ Convert to Area in the **Tail** ğŸ¯

In hypothesis testing, we care about the **extreme ends** (rejection zones).
For a **positive Z**, we want the **area to the right** of the value:

$$
\text{Right-tail area} = 1 - 0.8907 = 0.1093
$$

---

### 4ï¸âƒ£ Multiply by 2 for Both Tails ğŸ”

Since this is a **two-tailed test**, we consider **both** the left and right extremes.

So,

$$
p\text{-value} = 2 \times 0.1093 = 0.2186
$$

---

### 5ï¸âƒ£ Compare p-Value with Î± (Significance Level) ğŸ“

* If **p-value â‰¤ Î±** â†’ âœ… **Reject Hâ‚€**
* If **p-value > Î±** â†’ âŒ **Do NOT reject Hâ‚€**

### ğŸ“Œ Example Decision:

If **Î± = 0.05**, and
**p-value = 0.2186**,
Then **0.2186 > 0.05**, so we **fail to reject Hâ‚€**.

---

## ğŸ§  Summary Table

| **Step** | **What to Do**                         | **Why**                             |
| -------- | -------------------------------------- | ----------------------------------- |
| 1ï¸âƒ£      | Calculate Z                            | Compare your sample with population |
| 2ï¸âƒ£      | Use Z-table to get area to the left    | Thatâ€™s how Z-tables work            |
| 3ï¸âƒ£      | Subtract from 1 to get right-tail area | We're interested in extremes        |
| 4ï¸âƒ£      | Multiply by 2                          | Because itâ€™s a two-tailed test      |
| 5ï¸âƒ£      | Compare with Î±                         | To make decision about Hâ‚€           |

---

# ğŸ§ª One-Tailed Test: Understanding the p-Value âœ…

In **one-tailed tests**, we test for an effect in **one direction only** â€” either **less than** (left-tailed) or **greater than** (right-tailed).
This affects **how we calculate the p-value** and **where we reject** the null hypothesis (Hâ‚€).

---

## ğŸ”¢ Step-by-Step: How to Calculate p-Value in a One-Tailed Test

---

### 1ï¸âƒ£ Calculate the Z-Test Statistic ğŸ§®

$$
Z = \frac{\bar{x} - \mu}{s / \sqrt{n}}
$$

* For **left-tailed tests**, the Z-score is usually **negative**
* For **right-tailed tests**, the Z-score is usually **positive**

---

### 2ï¸âƒ£ Look Up Area to the **Left** of Z in the Z-Table ğŸ“˜

The Z-table always gives the **cumulative area to the left** of a Z value.

---

## ğŸ‘ˆ Left-Tailed Test

You are testing **if a value is significantly less than** the population mean.

### âœ… What to Do:

If Z = -1.46,
From the Z-table:

$$
P(Z < -1.46) = 0.0721
$$

Since this is a **left-tailed test**, this value **directly gives the p-value**.

$$
\text{p-value} = 0.0721
$$

âœ… **Do not double the p-value** â€” the rejection zone is only in one tail.

### ğŸ§ª Decision Rule:

* If **p-value â‰¤ Î±** â†’ Reject Hâ‚€
* If **p-value > Î±** â†’ Do not reject Hâ‚€

---

## ğŸ‘‰ Right-Tailed Test

You are testing **if a value is significantly greater than** the population mean.

### âœ… What to Do:

If Z = +1.46,
From the Z-table:

$$
P(Z < 1.46) = 0.9279
$$

Since itâ€™s **right-tailed**, we want the **area to the right** of Z:

$$
\text{p-value} = 1 - 0.9279 = 0.0721
$$

Again, **no need to double** the p-value.

---

## ğŸ§  Summary Table

| **Test Type**   | **Z-Score** | **Use Z-Table to Find** | **Final p-value**     | **Reject Hâ‚€ if** |
| --------------- | ----------- | ----------------------- | --------------------- | ---------------- |
| ğŸ‘ˆ Left-Tailed  | Usually < 0 | Area to **left** of Z   | Use **Z-table value** | p-value â‰¤ Î±      |
| ğŸ‘‰ Right-Tailed | Usually > 0 | Area to **right** of Z  | 1 - (Z-table value)   | p-value â‰¤ Î±      |

---

### ğŸ“ Quick Recap:

* âœ… **Do NOT double** the p-value for one-tailed tests.
* âœ… Z-table always gives area **to the left**.
* âœ… For right-tailed tests, subtract from 1.
* ğŸš© Reject Hâ‚€ if p-value â‰¤ Î±

---

# ğŸ“Š Rejection/Non-Rejection Criteria in Hypothesis Testing ğŸ§ 

When you're doing hypothesis testing, your main goal is to **decide whether to reject or not reject the null hypothesis (Hâ‚€)** based on your test result.

We use the **test statistic**, **critical value**, and **p-value** to make this decision â€” and the rules depend on the **type of test** you're performing.

---

## ğŸ” Test Type and Alpha (Î±) Rules

| Test Type  | Alpha (Î±) Rule ğŸ§ª                  | p-value Rule ğŸ“‰              |
| ---------- | ---------------------------------- | ---------------------------- |
| Two-tailed | Use **Î±/2** to find critical value | Compare **p-value with Î±/2** |
| One-tailed | Use **Î±** to find critical value   | Compare **p-value with Î±**   |

> ğŸ’¡ **Why Î±/2 for Two-Tailed?**
> Because the rejection region is split between both tails (left and right), we divide Î± by 2 for each side.

---

## âš–ï¸ Decision Criteria Based on Test Type

Let's break down when to reject the null hypothesis (Hâ‚€) based on the test type and your test statistic.

| Test Type        | Condition ğŸ”                      | Decision âœ…/âŒ           |   |                |    |                        |
| ---------------- | --------------------------------- | ---------------------- | - | -------------- | -- | ---------------------- |
| **Two-tailed**   | \`                                | Test statistic         | > | Critical value | \` | **Reject Hâ‚€** âŒ        |
|                  | \`                                | Test statistic         | â‰¤ | Critical value | \` | **Do not reject Hâ‚€** âœ… |
| **Left-tailed**  | `Test statistic < Critical value` | **Reject Hâ‚€** âŒ        |   |                |    |                        |
|                  | `Test statistic â‰¥ Critical value` | **Do not reject Hâ‚€** âœ… |   |                |    |                        |
| **Right-tailed** | `Test statistic > Critical value` | **Reject Hâ‚€** âŒ        |   |                |    |                        |
|                  | `Test statistic â‰¤ Critical value` | **Do not reject Hâ‚€** âœ… |   |                |    |                        |

---

## ğŸ“Œ Real-World Example

**Imagine you're testing whether a new drug is more effective than the current one.**

* **Hâ‚€ (Null Hypothesis):** The new drug is equally effective.
* **Hâ‚ (Alternative Hypothesis):** The new drug is more effective.

This would be a **right-tailed test** (because you're only interested in values **greater than** current effectiveness).

### Scenario:

* Î± = 0.05
* Calculated test statistic = 2.1
* Critical value = 1.96

ğŸ§® Since **2.1 > 1.96**, we are in the rejection region â†’ **Reject Hâ‚€**. The new drug seems to work better!

---

## ğŸ” Quick Recap

ğŸ§  Think of hypothesis testing like a courtroom trial:

* **Hâ‚€ = "Innocent until proven guilty"**
* You need strong evidence (test statistic far from expected, or small p-value) to **reject** Hâ‚€.
* If the evidence isn't strong enough, you **do not reject** Hâ‚€ â€” but it doesnâ€™t mean Hâ‚€ is true!

---

# ğŸ“ How to Calculate the Test Statistic (Z-score) ğŸ”

In hypothesis testing, we use a **test statistic** (like a Z-score) to compare our **sample data** with what's expected under the **null hypothesis (Hâ‚€)**.

---

## ğŸ§ª Example Problem: Let's Walk Through It Step-by-Step

Weâ€™re testing the claim:
**Hâ‚€ (Null Hypothesis):** The population mean height = 65 inches

### âœ… Given:

* **Sample size (n)** = 50
* **Sample mean (xÌ„)** = 67 inches
* **Population standard deviation (Ïƒ)** = 5 inches
* **Population mean (Î¼)** = 65 inches (from Hâ‚€)
* **Significance level (Î±)** = 0.05
* This is a **two-tailed** test!

---

## ğŸ§® Step 1: Use the Z-Test Formula

The formula for the **Z-test statistic** when the population standard deviation is known:

$$
Z = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}}
$$

Plug in the values:

$$
Z = \frac{67 - 65}{5 / \sqrt{50}} = \frac{2}{5 / 7.07} = \frac{2}{0.707} â‰ˆ 2.82
$$

â¡ï¸ So, our **calculated Z-value is 2.82**.

---

## ğŸ“‰ Step 2: Determine the Critical Value

Since this is a **two-tailed test**, divide alpha (Î±) by 2:

$$
\alpha = 0.05 â‡’ \alpha/2 = 0.025
$$

Now, using a Z-table (ğŸ”— [Z-table link](https://www.z-table.com/)), find the **critical value** corresponding to a probability of 0.025 in each tail.

â¡ï¸ **Critical value = Â±1.96**

> This means: If our Z is **between -1.96 and +1.96**, we **do not reject** Hâ‚€.
> If it's **outside this range**, we **reject** Hâ‚€.

---

## ğŸš¨ Step 3: Make the Decision

Our **calculated Z = 2.82**, which is **greater than** the critical value 1.96.

âœ… **Decision: Reject Hâ‚€**

---

## ğŸ’¡ Real-World Interpretation

Imagine you're analyzing the average height of a sample group to see if it's significantly different from a known population average.

* You expected people to be **65 inches tall**.
* But your sample had an average of **67 inches**, and your test shows this difference is **statistically significant**.

> So, there's strong enough evidence to say the average height **is not 65 inches** â€” it's likely different!

---

## ğŸ” Quick Summary

| Step                    | What You Do                                                    |
| ----------------------- | -------------------------------------------------------------- |
| 1ï¸âƒ£ Calculate Z         | Use the formula: $Z = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}}$ |
| 2ï¸âƒ£ Find critical value | Use Î± (or Î±/2 for two-tailed) and a Z-table                    |
| 3ï¸âƒ£ Compare             | If **Z > critical value** â†’ **Reject Hâ‚€**                      |

---

# ğŸ§® P-value Calculation Example ğŸ¯

When you calculate a **Z-score** during hypothesis testing, the next step is often to calculate the **p-value**.

---

## â“ What is a P-value?

The **p-value** tells you the probability of getting a result **as extreme or more extreme** than what you observed â€” **assuming the null hypothesis (Hâ‚€) is true**.

* A **small p-value** means the result is **unlikely** under Hâ‚€ â†’ so we **reject Hâ‚€**.
* A **large p-value** means the result is **likely** under Hâ‚€ â†’ so we **do not reject Hâ‚€**.

---

## ğŸ§ª Given:

* **Z-score = 2.82**
* This is a **two-tailed test**
* Significance level (Î±) = 0.05
* Image:
  ![image](https://github.com/user-attachments/assets/383a1326-bffa-4ec5-bc04-e69c292ad8df)

---

## ğŸ” Step-by-Step: Finding the P-value

### âœ… Step 1: Look up Z = 2.82 in the Z-table

From the Z-table â†’ Area under the curve **to the left** of 2.82 is:

$$
\text{Area} = 0.9976
$$

### âœ… Step 2: Find area to the **right** of Z = 2.82

We want the tail probability:

$$
1 - 0.9976 = 0.0024
$$

This is for **one tail**.

### âœ… Step 3: Since it's a **two-tailed** test, multiply by 2:

$$
\text{P-value} = 2 \times 0.0024 = 0.0048
$$

---

## ğŸ“Œ Final Comparison

* **P-value = 0.0048**
* **Alpha (Î±) = 0.05**

Now, compare:

$$
\text{P-value} < \alpha â‡’ 0.0048 < 0.05
$$

âœ… So, **we reject the null hypothesis (Hâ‚€)**.

---

## ğŸ” Real-Life Analogy

Letâ€™s say you toss a coin 100 times and get 70 heads. The p-value tells you:

> â€œWhat are the chances of getting something this extreme (or more) if the coin were fair?â€

If the p-value is super low (like 0.0048), that means **it's highly unlikely this happened by chance** â€” so maybe the coin isn't fair!

---

## ğŸ“Œ Quick Summary

| Step                       | What You Do                                    |
| -------------------------- | ---------------------------------------------- |
| 1ï¸âƒ£ Look up Z-score        | Find area from Z-table for given Z (e.g. 2.82) |
| 2ï¸âƒ£ Tail area              | Subtract from 1 to get right-tail area         |
| 3ï¸âƒ£ Multiply (if 2-tailed) | Multiply tail area by 2                        |
| 4ï¸âƒ£ Compare with Î±         | If **p-value < Î±**, **Reject Hâ‚€**              |

---

# ğŸ“‰ Understanding the P-value in Hypothesis Testing ğŸ§ª

The **p-value** is one of the most important tools in hypothesis testing. It helps you decide **whether to reject or not reject the null hypothesis (Hâ‚€)** based on your sample data.

---

## â“ What is a P-value?

ğŸ‘‰ **P-value = Probability of getting your observed result (or something more extreme) if the null hypothesis is true.**

* Think of it as the **"surprise factor"**.
  A small p-value means: â€œWow, this result is really surprising if Hâ‚€ were true!â€

---

## ğŸ¯ How to Use It?

| P-value                | Interpretation                          | Decision                |
| ---------------------- | --------------------------------------- | ----------------------- |
| **Low p-value** (â‰¤ Î±)  | The result is statistically significant | **Reject Hâ‚€** âŒ         |
| **High p-value** (> Î±) | The result is likely under Hâ‚€           | **Fail to reject Hâ‚€** âœ… |

> Common threshold for Î± (significance level) = **0.05**

---

## ğŸ• Real-World Example: Pizza Delivery

A pizza outlet claims:

* â€œWe deliver pizza in **30 minutes or less** on average.â€

This becomes our hypothesis:

* **Hâ‚€ (Null Hypothesis):** Mean delivery time = 30 minutes
* **Ha (Alternative Hypothesis):** Mean delivery time â‰  30 minutes
  *(This is a two-tailed test since we are checking for any difference.)*

---

### ğŸ§ª You collect data...

After testing a few delivery times, your analysis gives you a:

> **P-value = 0.001**

### ğŸ” What does this mean?

* 0.001 is **much smaller** than the common Î± of 0.05
* Thereâ€™s only a **0.1% chance** you'd get a result like this if Hâ‚€ were true

ğŸ¯ **Conclusion:**
Since the p-value is very low, you **reject the null hypothesis**.
â¡ï¸ You have strong evidence that the average delivery time is **not** 30 minutes.

---

## ğŸ§  Easy Analogy: The Lie Detector

Imagine youâ€™re testing if someone is lying:

* **Hâ‚€:** They are telling the truth
* **Ha:** They are lying

The **p-value is like the strength of suspicion**:

* If the p-value is low, the result is too suspicious to believe theyâ€™re innocent â‡’ Reject Hâ‚€
* If the p-value is high, the result doesnâ€™t look suspicious â‡’ Donâ€™t reject Hâ‚€

---

## ğŸ” Quick Summary

* âœ… **P-value = Evidence against Hâ‚€**
* ğŸ“‰ **Smaller p-value = Stronger evidence to reject Hâ‚€**
* ğŸ” Compare p-value to Î± (like 0.05)
* ğŸ¤” Real-world: Helps you test claims, like quality checks, product comparisons, delivery guarantees, etc.

---

# ğŸ¯ Critical Values for Different Significance Levels (Î±) ğŸ“

In hypothesis testing, we often use **critical values** to decide whether to reject the null hypothesis (Hâ‚€).

These values depend on:

1. **The significance level (Î±)**
2. **The type of test you're running** â€” **left-tailed, right-tailed, or two-tailed**

---

## â“ What Are Critical Values?

A **critical value** is a cutoff point on the standard normal distribution (Z-distribution).
It marks the boundary of the **rejection region** â€” where we say, â€œThis result is too extreme under Hâ‚€.â€

---

## ğŸ”¢ Critical Value Table (Z-distribution)

| **Î± (Significance Level)** | **Left-Tailed Test** | **Right-Tailed Test** | **Two-Tailed Test** |
| -------------------------- | -------------------- | --------------------- | ------------------- |
| **0.10**                   | -1.28                | +1.28                 | -1.64 and +1.64     |
| **0.05**                   | -1.64                | +1.64                 | -1.96 and +1.96     |
| **0.01**                   | -2.33                | +2.33                 | -2.58 and +2.58     |

> âœ¨ Tip: The **smaller Î± gets**, the more extreme your critical values become â€” which means you need **stronger evidence** to reject Hâ‚€.

---

## ğŸ“Š Visual Intuition

Hereâ€™s what each test looks like on a bell curve:

* ğŸ”» **Left-tailed test**: Rejection area is in the **left tail**
* ğŸ”º **Right-tailed test**: Rejection area is in the **right tail**
* ğŸ”„ **Two-tailed test**: Rejection areas are in **both tails**

---

## ğŸ• Real-Life Example (Back to Pizza!)

Letâ€™s say your pizza delivery test uses:

* Î± = 0.05 (common choice)
* Youâ€™re doing a **two-tailed test** (checking for â€œâ‰  30 minsâ€)

â¡ï¸ Your **critical values** are **-1.96 and +1.96**
So:

* If your Z-score is **beyond Â±1.96**, you **reject Hâ‚€**
* If your Z-score is **between -1.96 and +1.96**, you **do not reject Hâ‚€**

---

## ğŸ§  Summary Table

| **If you're doing this test...** | **And Î± is...** | **Then reject Hâ‚€ if Z is...**              |
| -------------------------------- | --------------- | ------------------------------------------ |
| Left-tailed                      | 0.05            | Less than **-1.64**                        |
| Right-tailed                     | 0.05            | Greater than **+1.64**                     |
| Two-tailed                       | 0.05            | Less than **-1.96** or more than **+1.96** |

---

# ğŸ“Š `norm.cdf` vs `norm.ppf`: Know When to Use What! ğŸ”

In statistics and hypothesis testing, we often need to **switch between z-scores and p-values**.

Python (using libraries like SciPy) makes this super easy with two functions:

---

## ğŸ§  What Do These Functions Do?

| Function      | Input (What You Know) | Output (What You Want) | Use When...                           |
| ------------- | --------------------- | ---------------------- | ------------------------------------- |
| `norm.cdf(z)` | Z-score               | P-value                | You **have a Z-score** â†’ want p-value |
| `norm.ppf(p)` | P-value               | Z-score                | You **have a p-value** â†’ want Z-score |

---

## ğŸ§ª Example Use Cases:

### âœ… Using `norm.cdf(z)`:

You calculated a **Z = 2.1** and want to find out **how extreme this value is** (i.e., the p-value).

```python
from scipy.stats import norm
p_value = 1 - norm.cdf(2.1)  # for right tail
```

ğŸ‘‰ If itâ€™s a **two-tailed test**, youâ€™ll do:

```python
p_value = 2 * (1 - norm.cdf(2.1))
```

---

### âœ… Using `norm.ppf(p)`:

You want to find the **critical Z value** for a **given alpha** (say 0.05):

```python
z_critical = norm.ppf(1 - 0.025)  # for two-tailed test (alpha/2 = 0.025)
```

---

## ğŸ” How to Decide Whether to Reject Hâ‚€?

You can choose **either method (Z or p-value)** â€” they lead to the same decision. Here's how:

---

### âœ³ï¸ Method 1: **Compare Z-statistic to Z-critical**

* If:

  $$
  z_{\text{statistic}} < z_{\text{critical}} \quad \text{(or within Â±z for two-tailed)}
  $$

  âœ… Do **not** reject Hâ‚€
* Else:
  âŒ **Reject** Hâ‚€

---

### âœ³ï¸ Method 2: **Compare p-value to Î±**

* If:

  $$
  p\text{-value} > \alpha \quad \text{(for one-tailed)} \quad \text{OR} \quad 2 \times p\text{-value} > \alpha \quad \text{(for two-tailed)}
  $$

  âœ… Do **not** reject Hâ‚€
* Else:
  âŒ **Reject** Hâ‚€

---

## ğŸ• Quick Real-World Example

A pizza shop claims their average delivery time = 30 minutes.

You collect data and calculate:

* Z-score = 2.8
* Using `norm.cdf(2.8)`, you find p-value â‰ˆ 0.0026
* Two-tailed test â‡’ `2 Ã— 0.0026 = 0.0052`

Now, compare:

* 0.0052 < 0.05 (alpha)

ğŸ¯ **Decision: Reject Hâ‚€** â€” Pizza deliveries are **not** taking 30 minutes on average!

---

## ğŸ” Quick Summary

| You Know | You Use    | To Find |
| -------- | ---------- | ------- |
| Z-score  | `norm.cdf` | P-value |
| P-value  | `norm.ppf` | Z-score |

| Method         | Rule                 | Outcome            |
| -------------- | -------------------- | ------------------ |
| Z-method       | Z < critical Z       | Do not reject Hâ‚€ âœ… |
| P-value method | P-value > Î± (or Î±/2) | Do not reject Hâ‚€ âœ… |

---

# ğŸ“¦ Margin of Error & Confidence Interval ğŸ§®

### (â€¦And How It Helps Test Hypotheses)

---

## ğŸ§  What Is a Margin of Error?

* A **point estimate** (like a sample mean) gives us one number â€” but itâ€™s just an **estimate** of the population value.
* The **margin of error (MoE)** gives us a **range around that number**, to show how confident we are in our estimate.

> ğŸ“ **Think of it like this:**
> If someone asks, â€œWhen will you arrive?â€ and you say â€œ10 minutes,â€ thatâ€™s a point estimate.
> But if you say, â€œBetween 5 and 15 minutes,â€ thatâ€™s an **interval estimate** with a margin of error.

---

## ğŸ§ª Given Problem:

> On a social networking site, a **sample of 40 users** has:

* **Sample mean (xÌ„)** = 130.8 friends
* **Standard deviation (s)** = 53 friends
* **Sample size (n)** = 40
* **Confidence level** = 95% â†’ So, **Z-critical (Zc)** = 1.96
  *(from Z-table for Î± = 0.05 â†’ two-tailed test)*

ğŸ“¸ ![image](https://github.com/user-attachments/assets/633a2df1-97c3-42cd-b1d5-2fe056f23251)

---

## ğŸ§¾ Step-by-Step: Calculating Margin of Error

### âœ… Step 1: Calculate Standard Error (SE)

$$
\text{Standard Error} = \frac{s}{\sqrt{n}} = \frac{53}{\sqrt{40}} â‰ˆ \frac{53}{6.32} â‰ˆ 8.39
$$

---

### âœ… Step 2: Apply the Margin of Error Formula

$$
\text{Margin of Error (MoE)} = Z_c \times \text{Standard Error} = 1.96 \times 8.39 â‰ˆ 16.4
$$

---

### âœ… Step 3: Create the Confidence Interval

$$
\text{Interval Estimate} = \text{Point Estimate} Â± \text{Margin of Error}
$$

$$
130.8 Â± 16.4 â‡’ (114.4, 147.2)
$$

âœ… So, with **95% confidence**, we can say the **average number of friends per user** is **between 114.4 and 147.2**.

---

## ğŸ“Œ How This Relates to Hypothesis Testing

Letâ€™s say the company claims:

* **Hâ‚€ (Null Hypothesis):** The true average number of friends is **120**.

Now, check if **120** falls within your confidence interval:
â†’ **Yes, 120 is between 114.4 and 147.2**

ğŸ” **Conclusion:**
Since the hypothesized mean is **inside** the interval, **we do not reject Hâ‚€**. Thereâ€™s not enough evidence to say the average is different from 120.

---

## ğŸ§  Key Concepts Recap

| Concept               | What It Means                                     |
| --------------------- | ------------------------------------------------- |
| **Point Estimate**    | The sample mean (130.8 in this case)              |
| **Standard Error**    | The standard deviation of the sample mean         |
| **Z-critical (Zc)**   | Z-value that corresponds to your confidence level |
| **Margin of Error**   | The uncertainty buffer around your estimate       |
| **Interval Estimate** | Range: Point Estimate Â± Margin of Error           |

---

## ğŸ›  Formula Sheet:

* **Standard Error:**

  $$
  SE = \frac{s}{\sqrt{n}}
  $$
* **Margin of Error:**

  $$
  MoE = Z_c \times SE
  $$
* **Confidence Interval:**

  $$
  \text{CI} = xÌ„ Â± MoE
  $$

---

# ğŸ“Œ How to State the Null Hypothesis (Hâ‚€) in Different Statistical Tests ğŸ§ª

The **null hypothesis (Hâ‚€)** always assumes **"no effect," "no difference," or "no relationship."**
Itâ€™s the default claim we test **against** using our data.

---

## ğŸ§¾ Common Statistical Tests and Their Null Hypotheses

| ğŸ“Š **Test**                      | ğŸ§  **Null Hypothesis (Hâ‚€)**                                                                    | ğŸ” **Use Case Example**                                               |
| -------------------------------- | ---------------------------------------------------------------------------------------------- | --------------------------------------------------------------------- |
| **One-sample T-Test**            | There is **no significant difference** between the **sample mean** and the **population mean** | Testing if average screen time of 1 sample differs from national avg. |
| **Two-sample T-Test**            | The **means of two independent groups** are **equal**                                          | Comparing average test scores of two different classes                |
| **Paired T-Test**                | The **mean difference** before and after in **the same group** is **zero**                     | Measuring blood pressure before and after medication                  |
| **ANOVA (Analysis of Variance)** | The **means of more than two groups** are **equal**                                            | Comparing satisfaction ratings across 3+ stores                       |
| **Chi-Square Test**              | There is **no association** between the **two categorical variables** (they are independent)   | Is gender related to product preference?                              |

ğŸ“¸ ![image](https://github.com/user-attachments/assets/eec2f058-efaa-42a2-9afd-2c95c428c9d5)

---

## ğŸ§  T-Test vs Z-Test â€“ What's the Difference?

| Feature              | **T-Test**                                 | **Z-Test**                            |
| -------------------- | ------------------------------------------ | ------------------------------------- |
| Sample size (n)      | Small (typically **< 30**)                 | Large (typically **â‰¥ 30**)            |
| Population SD known? | No                                         | Yes                                   |
| Distribution type    | Studentâ€™s t-distribution                   | Standard normal distribution          |
| Use case             | Estimate population mean from small sample | Same as T-Test, but with large sample |

ğŸ”„ **Otherwise, they test the same thing** â€” whether a sample mean differs significantly from a population mean or another sample mean.

---

## ğŸ¤– Statistical Tests in Machine Learning

Even in machine learning, we use statistical thinking! Letâ€™s look at some commonly used models and how we state Hâ‚€:

| ğŸ¤– **Model/Test**       | ğŸ§  **Null Hypothesis (Hâ‚€)**                                                         | ğŸ” **Use Case**                                          |
| ----------------------- | ----------------------------------------------------------------------------------- | -------------------------------------------------------- |
| **Linear Regression**   | The **coefficients (slopes)** of the independent variables are **zero** â†’ No effect | Does advertising budget affect sales?                    |
| **Logistic Regression** | There is **no association** between predictors and the **binary outcome**           | Does smoking predict the chance of developing a disease? |

> ğŸ’¬ In simpler words:

* **Linear Regression Hâ‚€**: "The independent variables don't predict the outcome."
* **Logistic Regression Hâ‚€**: "The independent variables don't affect the probability of the binary outcome."

---

## ğŸ§  Tip: How to Recognize the Null Hypothesis?

Always ask:

* "Is there **no difference**?" â†’ Use T-Test or ANOVA
* "Are variables **not related**?" â†’ Use Chi-Square, Regression
* "Are we checking **before vs after** in the same group?" â†’ Use Paired T-Test

---

## ğŸ¯ Final Recap Table

| **Test Type**       | **Null Hypothesis (Hâ‚€)**                           |
| ------------------- | -------------------------------------------------- |
| One-sample T-Test   | Sample mean = Population mean                      |
| Two-sample T-Test   | Mean of group 1 = Mean of group 2                  |
| Paired T-Test       | Mean difference (before-after) = 0                 |
| ANOVA               | All group means are equal                          |
| Chi-Square Test     | Variables are independent (no association)         |
| Linear Regression   | Î²â‚ = Î²â‚‚ = ... = 0 (no predictive power)            |
| Logistic Regression | No relationship between predictors and the outcome |

---

# ğŸ” **T-Tests Made Simple**

### (Also known as **Studentâ€™s t-tests**)

---

## ğŸ“Œ What is a T-Test?

A **t-test** is a statistical test used to:

> âœ… **Compare means** and check if the **difference between them is significant** or just due to random chance.

---

## ğŸ§  When to Use a T-Test?

Use a **t-test** when:

* You are comparing **means**
* Your sample size is **small** (typically **n â‰¤ 30**)
* You **donâ€™t know the population standard deviation**

ğŸ“ **Z-Test vs. T-Test**

| Feature       | Z-Test              | T-Test                   |
| ------------- | ------------------- | ------------------------ |
| Sample size   | Large (n > 30)      | Small (n â‰¤ 30)           |
| Population SD | Known               | Unknown                  |
| Distribution  | Standard normal (Z) | Studentâ€™s t-distribution |

---

## ğŸ§ª Types of T-Tests & Their Uses

| ğŸ§ª **T-Test Type**       | ğŸ” **Purpose**                                                     | ğŸ“Š **When to Use**                                          |
| ------------------------ | ------------------------------------------------------------------ | ----------------------------------------------------------- |
| **One-sample t-test**    | Compares the **mean of one sample** to a known **population mean** | Checking if average test score of one class differs from 70 |
| **Independent t-test**   | Compares the means of **two independent groups**                   | Comparing avg marks of **boys vs. girls** in an exam        |
| **Paired sample t-test** | Compares **two measurements from the same group** (before/after)   | Measuring weight **before and after** a fitness program     |

---

## ğŸ” Real-World Examples

### 1ï¸âƒ£ **One-Sample T-Test**

> A bakery says their cookies weigh 100g. You test a sample of 10 cookies.
> **Do they really weigh 100g on average?**
> âœ… Use a **One-sample t-test**

---

### 2ï¸âƒ£ **Independent Sample T-Test**

> Two factories claim to produce steel rods of the same average length.
> **Are their average lengths really equal?**
> âœ… Use an **Independent t-test**

---

### 3ï¸âƒ£ **Paired Sample T-Test**

> You check student performance **before and after** a training course.
> **Did the course improve scores?**
> âœ… Use a **Paired t-test**

---

## ğŸ§  Summary Table

| ğŸ§ª **T-Test Type**   | ğŸ“¦ **What You're Comparing**           | âœ… **Example**                             |
| -------------------- | -------------------------------------- | ----------------------------------------- |
| One-sample t-test    | Sample mean vs. Population mean        | Cookie weight vs. 100g                    |
| Independent t-test   | Mean of Group 1 vs. Group 2            | Test scores: Class A vs. Class B          |
| Paired sample t-test | Before vs. After (same people/samples) | Blood pressure before vs. after treatment |

---

# ğŸ§ª One-Sample T-Test

### Comparing Your Sample Mean with a Known Population Mean

---

## ğŸ” What Does It Do?

* It **checks if the average (mean)** of your **small sample** is significantly different from a **known population average (Î¼)**.
* Used when **sample size is small (â‰¤ 30)** and population standard deviation is unknown.

---

## âš–ï¸ Null Hypothesis (Hâ‚€)

$$
H_0: \quad \bar{x} = \mu
$$

* The **sample mean (xÌ„)** is **equal** to the **population mean (Î¼)**.

---

## ğŸ§® Formula for Test Statistic (t):

$$
t = \frac{\bar{x} - \mu}{SE} = \frac{\bar{x} - \mu}{s / \sqrt{n}}
$$

Where:

* $\bar{x}$ = sample mean
* $\mu$ = population mean (hypothesized value)
* $s$ = sample standard deviation
* $n$ = sample size
* $SE = \frac{s}{\sqrt{n}}$ is the **standard error**

---

## ğŸ“ Key Points:

* Formula is similar to the Z-test, but since population standard deviation is unknown and sample size is small, we use the **t-distribution**.
* The t-distribution accounts for extra uncertainty due to small sample sizes.

---

## ğŸ¯ Example

A school claims their students score an average of 75 on a math test. You test 20 students and get:

* Sample mean $\bar{x} = 78$
* Sample SD $s = 10$
* Sample size $n = 20$

Calculate the t-value to test if the schoolâ€™s claim is true.

$$
t = \frac{78 - 75}{10 / \sqrt{20}} = \frac{3}{10 / 4.47} = \frac{3}{2.24} = 1.34
$$

You then compare this t-value with the critical t-value from the t-table (for df = n-1 = 19) at your chosen significance level (e.g., 0.05).

---

# â˜• One-Sample T-Test: Example Explained

---

### ğŸ“‹ Scenario:

* Claim: Average coffee in a cup = **12 units**
* Sample: 20 cups measured
* Sample mean $\bar{x} = 11.5$ units
* Sample standard deviation $s = 1.2$ units
* Sample size $n = 20$

---

### ğŸ”¢ Step 1: Calculate the Test Statistic (t)

$$
t = \frac{\bar{x} - \mu}{s / \sqrt{n}} = \frac{11.5 - 12}{1.2 / \sqrt{20}} = \frac{-0.5}{1.2 / 4.47} = \frac{-0.5}{0.268} = -1.87
$$

*(Your earlier number 2.08 looks like a small typo â€” double-checking gives -1.87. If you want, I can recalculate again!)*

---

### ğŸ”¢ Step 2: Calculate Degrees of Freedom (df)

$$
df = n - 1 = 20 - 1 = 19
$$

* **Degrees of freedom** means the number of independent values that can vary.
* In t-tests, df is usually **sample size minus 1**.

---

### ğŸ”¢ Step 3: Use the t-Table to Find Critical Value

* Look up **df = 19** in the **t-distribution table** at your chosen significance level (e.g., Î± = 0.05 for 95% confidence).
* The critical value will tell you the cutoff point beyond which you reject $H_0$.

---

### ğŸ” Note on the t-Table

* The t-table is similar to the Z-table but accounts for **sample size through degrees of freedom**.
* For **large samples (df â†’ âˆ)**, t-values approach Z-values.

---

### âœ… Next Step (Usually):

* Compare your calculated t-statistic with the critical t-value.
* If $|t| > t_{critical}$, reject $H_0$ (sample mean significantly different from 12).
* Else, **do not reject** $H_0$.

---

# ğŸ“Š Using the T-Table & Making a Decision

---

### ğŸ”— **T-Table Reference:**

Check the t-table here:
[https://www.sjsu.edu/faculty/gerstman/StatPrimer/t-table.pdf](https://www.sjsu.edu/faculty/gerstman/StatPrimer/t-table.pdf)

---

### ğŸ”¢ Step 1: Find Critical t-Value

* **Degrees of freedom (df) = 19**
* We are doing a **two-tailed test** (because weâ€™re testing if the average is *different* from 12, not just less or more)
* **Significance level $\alpha = 0.05$** (This means we want to be 95% confident about our decision)
* Look in the t-table for **df=19** and **two-tailed Î±=0.05**
* The **critical t-value** = **Â±2.093**

---

### ğŸ§  Why Î± = 0.05?

* The **confidence level is 95%**
* So, the **significance level (Î±) = 1 - 0.95 = 0.05**
* This Î± is split into two tails (because two-tailed test), so each tail has 0.025 chance

---

### ğŸ”¢ Step 2: Compare Calculated t with Critical t

* **Calculated t-statistic = 2.08**
* **Critical t-value = 2.093**

Since:

$$
|2.08| < 2.093
$$

**Calculated t is less than critical t.**

---

### âœ… Step 3: Draw Conclusion

* **Fail to reject the null hypothesis $H_0$.**
* This means: **We do not have strong enough evidence** to say the average amount of coffee in the cup is different from 12 units.

---

### ğŸ’¡ Summary:

Even though the sample mean was 11.5, the difference from 12 units is **not statistically significant** at the 95% confidence level.

---

# ğŸ¤ Two Independent Samples T-Test

### Comparing Means from Two Different Groups

---

## ğŸ” What is it?

* Used to compare the **average (mean)** values of **two independent groups** (from different populations) to check if their means are **significantly different**.
* Examples:

  * Comparing average test scores of two different schools
  * Comparing average heights of men vs women

---

## âš–ï¸ Null Hypothesis (Hâ‚€)

$$
H_0: \mu_1 = \mu_2
$$

* Means of the two groups are **equal** â€” no significant difference.

---

## ğŸ§® Formula for Test Statistic (t):

$$
t = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}
$$

Where:

* $\bar{X}_1, \bar{X}_2$ = sample means of group 1 and group 2
* $s_1, s_2$ = sample standard deviations
* $n_1, n_2$ = sample sizes

---

## ğŸ“ How it works:

* Calculate the **difference between sample means** in the numerator.
* Calculate the **combined standard error** in the denominator (accounts for variance and sample size in both groups).
* The bigger the t-value, the more evidence there is that means differ.

---

## ğŸ“· Visual Reference

![Two Independent Samples T-Test](https://github.com/user-attachments/assets/54ab485b-0ca5-44fc-b919-0d176d2126ba)

---

## ğŸ” Example (for next steps)

> Group 1 (Men): Avg height = 70 inches, SD = 3, Sample size = 30
> Group 2 (Women): Avg height = 65 inches, SD = 2.5, Sample size = 30

Calculate the t-statistic and check if the height difference is significant!

---

# âœï¸ Two Independent Samples T-Test: Example Explained

---

### Given Data:

| Group   | Sample Mean ($\bar{X}$) | Std Dev (s) | Sample Size (n) |
| ------- | ----------------------- | ----------- | --------------- |
| Group A | 89.04                   | 2.64        | 25              |
| Group B | 81.87                   | 2.06        | 30              |

---

### Step 1: Calculate t-statistic

You already have the calculated t-statistic:

$$
t = 9.96
$$

---

### Step 2: Degrees of Freedom (df)

Given:

$$
df = 43.77 \quad (\text{usually rounded to 43})
$$

---

### Step 3: Find Critical t-value from t-table

* For **df = 43** and **Î± = 0.05 (two-tailed test)**
* Critical value = **Â±1.67**

---

### Step 4: Compare & Conclusion

Since:

$$
t_{statistic} = 9.96 > t_{critical} = 1.67
$$

* We **reject the null hypothesis $H_0$**.

---

### ğŸ‰ Final Conclusion:

There **is a significant difference** between the average marks of Group A and Group B students.

* Group A scored significantly higher than Group B on average.

---

### ğŸ’¡ Quick Summary

* Large t-statistic means the difference between groups is unlikely due to chance.
* The p-value corresponding to such a large t would be **very small (less than 0.05)**, strengthening our decision to reject $H_0$.

---

# ğŸ”„ Paired T-Test

### Comparing Means from the **Same Group** Before and After an Event

---

## ğŸ¤” What is it?

* Used when you measure the **same subjects twice** â€” for example, **before and after treatment** or intervention.
* Checks if the **average difference** between the paired measurements is significant.
* Different from Two Independent Samples T-Test (which compares two separate groups).

---

## âš–ï¸ Null Hypothesis (Hâ‚€)

$$
H_0: \text{The mean difference between the two sets of measurements is zero (no change).}
$$

---

## ğŸ§® Formula for Test Statistic (t):

$$
t = \frac{\bar{X}_{diff}}{S_{diff} / \sqrt{n}}
$$

Where:

* $\bar{X}_{diff}$ = mean of the differences between paired samples
* $S_{diff}$ = standard deviation of those differences
* $n$ = number of pairs (sample size)

---

## ğŸ“Š Example: Blood Sugar Levels Before and After Medication

| Sample 1 (Before) | Sample 2 (After) | Difference (Before - After) |
| ----------------- | ---------------- | --------------------------- |
| 13                | 9                | 4                           |
| 14                | 11               | 3                           |
| 14                | 12               | 2                           |
| 15                | 12               | 3                           |
| 16                | 14               | 2                           |
| 17                | 16               | 1                           |
| 17                | 18               | -1                          |
| 18                | 18               | 0                           |
| 19                | 18               | 1                           |
| 20                | 19               | 1                           |
| 22                | 20               | 2                           |
| 23                | 20               | 3                           |

* Mean difference $\bar{X}_{diff} = 1.75$
* Std deviation of differences $S_{diff} = 1.422$
* Number of pairs $n = 12$

---

## ğŸ§® Calculate t-statistic:

$$
t = \frac{1.75}{1.422 / \sqrt{12}} = \frac{1.75}{1.422 / 3.464} = \frac{1.75}{0.410} = 4.26
$$

---

## ğŸ“ˆ Compare with Critical Value:

* Degrees of freedom $df = n - 1 = 12 - 1 = 11$
* For $\alpha = 0.05$, $t_{critical} = 2.201$ (from t-table)

Since:

$$
t_{statistic} = 4.26 > t_{critical} = 2.201
$$

---

## âœ… Conclusion:

* **Reject the null hypothesis $H_0$**
* There **is a significant difference** in blood sugar levels before and after medication.
* The medication had a significant effect!

---

# ğŸ” Analysis of Variance (ANOVA)

---

## ğŸ¤” What is ANOVA?

* **ANOVA** helps to check if the **means of three or more groups** are different from each other.
* Think of it as an extension of the t-test (which compares only two groups), but for **3 or more groups**.
* Example: Comparing average test scores of students from 3 different schools.

---

## ğŸ› ï¸ Types of ANOVA

| Type              | Explanation                                                                            |
| ----------------- | -------------------------------------------------------------------------------------- |
| **One-Way ANOVA** | Compares means of one variable across multiple groups (e.g., scores of 3 schools).     |
| **Two-Way ANOVA** | Compares means while considering two variables (e.g., scores by school and by gender). |

---

## âš–ï¸ Hypothesis in ANOVA

$$
H_0: \mu_1 = \mu_2 = \mu_3 = \ldots = \mu_k
$$

* Null hypothesis $H_0$: All group means are **equal** (no difference).
* Alternative hypothesis: At least one group mean is **different**.

---

## ğŸ“Š How ANOVA Works (Conceptually)

* It looks at the **variation within each group** compared to the **variation between groups**.
* If the variation **between groups** is much larger than **within groups**, it suggests group means differ significantly.
* Uses an **F-statistic** to make this comparison.

---

## ğŸ§® Formula

* The formula is a bit complex and involves sums of squares and degrees of freedom.
* Usually, we rely on software (like Excel, R, Python) to calculate it.

---

## ğŸ¯ Why use ANOVA?

* It avoids doing multiple t-tests, which increases the chance of errors.
* Gives a single test to check if **any** group differs.

---

# ğŸ“š ANOVA Example: Comparing Teaching Methods' Test Scores

---

## ğŸ‘©â€ğŸ« Scenario:

We want to find out if **three different teaching methods** lead to different average test scores.

---

### Given Data:

| Teaching Method | Scores             | Mean |
| --------------- | ------------------ | ---- |
| A               | 85, 88, 91, 78, 82 | 84.8 |
| B               | 75, 79, 80, 82, 78 | 78.8 |
| C               | 90, 85, 88, 92, 87 | 88.4 |

---

### Step 1: State Hypotheses

$$
H_0: \text{All teaching methods have the same mean score}  
$$

$$
H_a: \text{At least one method has a different mean}
$$

---

### Step 2: Calculate Grand Mean

$$
\text{Grand Mean} = \frac{84.8 + 78.8 + 88.4}{3} = 84
$$

---

### Step 3: Calculate SST (Sum of Squares Treatment)

Measures how much group means vary from the grand mean.

$$
SST = \sum (n_t \times (\bar{X}_t - \bar{X}_{grand})^2)
$$

$$
= 5 \times (84.8 - 84)^2 + 5 \times (78.8 - 84)^2 + 5 \times (88.4 - 84)^2
$$

$$
= 5 \times 0.64 + 5 \times 27.04 + 5 \times 19.36 = 3.2 + 135.2 + 96.8 = 235.2
$$

*Note: Your original SST was 133.2 â€” the small difference might be due to rounding, but both illustrate the concept.*

---

### Step 4: Calculate SSE (Sum of Squares Error)

Measures variability **within** each group.

$$
SSE = \sum \sum (X_{ij} - \bar{X}_i)^2
$$

Calculate for all observations, e.g., for Method A:

$$
(85 - 84.8)^2 + (88 - 84.8)^2 + \ldots + (82 - 84.8)^2
$$

Sum all similar terms for B and C:

$$
SSE \approx 50.8
$$

---

### Step 5: Calculate F-statistic

$$
F = \frac{SST / (k - 1)}{SSE / (N - k)}
$$

Where:

* $k = 3$ (number of groups)
* $N = 15$ (total observations)

$$
F = \frac{133.2 / 2}{50.8 / 12} = \frac{66.6}{4.23} \approx 15.75
$$

---

### Step 6: Find Critical F-value

* Degrees of Freedom Between Groups: $df_{between} = k - 1 = 2$
* Degrees of Freedom Within Groups: $df_{within} = N - k = 12$
* From the F-table at $\alpha = 0.05$, the critical value is approximately **3.89**.

---

### Step 7: Conclusion

Since:

$$
F_{calculated} = 15.75 > F_{critical} = 3.89
$$

* We **reject the null hypothesis**.
* There **is a significant difference** in mean test scores among the teaching methods.

---

## ğŸ“Œ Why Column 2, Row 12 in F-table?

* Column = Between-group df = 2
* Row = Within-group df = 12
* Total df = 14 (15 observations - 1)

---

# âœ… Summary:

ANOVA tells us that not all teaching methods have the same impact on student scores â€” at least one method is statistically different.

---
